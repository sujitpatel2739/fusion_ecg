{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njE_q6If8iRt"
      },
      "source": [
        "# ECG Classification - Complete Project in Colab\n",
        "## Multi-Modal Deep Learning for ECG Analysis\n",
        "\n",
        "**Your complete original project - no modifications**\n",
        "\n",
        "This notebook contains ALL your code exactly as you wrote it:\n",
        "- ✅ Your custom RNN models (GRU/LSTM with attention)\n",
        "- ✅ Your custom CNN2D models (AlexNet-inspired with attention)\n",
        "- ✅ Your custom ResNet and VGG implementations\n",
        "- ✅ Your fusion models\n",
        "- ✅ Your data loaders with caching\n",
        "- ✅ Your training pipeline\n",
        "- ✅ Your metrics and evaluation\n",
        "- ✅ Everything from your project files\n",
        "\n",
        "**5-class ECG Classification**: NORM, MI, STTC, CD, HYP  \n",
        "**Dataset**: PTB-XL from PhysioNet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fAksPN5E8iRu"
      },
      "source": [
        "## 1. Setup & Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3L7l5DM8iRu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "de96f347-3cd8-43b1-ade8-cadf4e7f1dec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Running on Google Colab\n",
            "\n",
            "PyTorch version: 2.9.0+cpu\n",
            "CUDA available: False\n",
            "⚠ No GPU detected - training will be slow!\n"
          ]
        }
      ],
      "source": [
        "# Check environment and GPU\n",
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "    print(\"✓ Running on Google Colab\")\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "    print(\"✗ Not running on Colab\")\n",
        "\n",
        "import torch\n",
        "print(f\"\\nPyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU Device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
        "else:\n",
        "    print(\"⚠ No GPU detected - training will be slow!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFmn3vSq8iRv"
      },
      "outputs": [],
      "source": [
        "# Install dependencies\n",
        "!pip install -q torch torchvision torchaudio\n",
        "!pip install -q numpy pandas matplotlib seaborn scikit-learn tqdm\n",
        "!pip install -q wfdb pyts  # For PTB-XL and time series conversion\n",
        "\n",
        "print(\"✓ Dependencies installed successfully\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9sfuyKFI8iRv"
      },
      "outputs": [],
      "source": [
        "# Setup storage for collab\n",
        "if IN_COLAB:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    SAVE_DIR = '/content/drive/MyDrive/ECG_Checkpoints/'\n",
        "else:\n",
        "    SAVE_DIR = 'checkpoints/'\n",
        "\n",
        "import os\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "os.makedirs('results', exist_ok=True)\n",
        "\n",
        "# Create data directories\n",
        "data_dirs = [\n",
        "    'data/signals/train', 'data/signals/validation', 'data/signals/test',\n",
        "    'data/images/train/gaf', 'data/images/train/mtf',\n",
        "    'data/images/validation/gaf', 'data/images/validation/mtf',\n",
        "    'data/images/test/gaf', 'data/images/test/mtf',\n",
        "    'data/labels'\n",
        "]\n",
        "for d in data_dirs:\n",
        "    os.makedirs(d, exist_ok=True)\n",
        "\n",
        "print(f\"✓ Directories created\")\n",
        "print(f\"  Checkpoints: {SAVE_DIR}\")\n",
        "print(f\"  Results: results/\")\n",
        "print(f\"  Data: data/\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x6rkXp1s8iRv"
      },
      "source": [
        "## 2. Configuration\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh1728xt8iRv"
      },
      "source": [
        "**From: `config.py`** - Your original configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M383ADKT8iRv",
        "outputId": "95552115-087a-47cd-d0ac-814c17ff1470"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Configuration loaded\n",
            "  Device: cuda\n",
            "  Batch size: 64\n",
            "  Epochs: 10\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import os\n",
        "\n",
        "# Configurations, Adjust Properly\n",
        "class Config:\n",
        "    # Data paths\n",
        "    TRAIN_SIGNAL_PATH = 'data/signals/train/'\n",
        "    TRAIN_IMAGE_PATH = 'data/images/train/'\n",
        "    TRAIN_LABEL_PATH = 'data/labels/y_train.npy'\n",
        "    VAL_SIGNAL_PATH = 'data/signals/validation/'\n",
        "    VAL_IMAGE_PATH = 'data/images/validation/'\n",
        "    VAL_LABEL_PATH = 'data/labels/y_val.npy'\n",
        "    TEST_SIGNAL_PATH = 'data/signals/test/'\n",
        "    TEST_IMAGE_PATH = 'data/images/test/'\n",
        "    TEST_LABEL_PATH = 'data/labels/y_test.npy'\n",
        "\n",
        "    METRICS_SAVE_PATH = 'saved_metrics'\n",
        "    if not os.path.exists(METRICS_SAVE_PATH):\n",
        "        os.makedirs(METRICS_SAVE_PATH)\n",
        "    TRAIN_HISTORY_SAVE_PATH = 'training_history'\n",
        "    if not os.path.exists(TRAIN_HISTORY_SAVE_PATH):\n",
        "        os.makedirs(TRAIN_HISTORY_SAVE_PATH)\n",
        "    EVAL_METRICS_SAVE_PATH = 'evaluation_metrics'\n",
        "    if not os.path.exists(EVAL_METRICS_SAVE_PATH):\n",
        "        os.makedirs(EVAL_METRICS_SAVE_PATH)\n",
        "\n",
        "    # Data loading parameters\n",
        "    SAVED_BATCH_SIZE = 32\n",
        "\n",
        "    # Model parameters\n",
        "    HIDDEN_SIZE = 128\n",
        "    NUM_LAYERS = 2\n",
        "    DROPOUT = 0.3\n",
        "\n",
        "    # Optimization\n",
        "    WEIGHT_DECAY = 1e-4  # L2 regularization\n",
        "    PATIENCE = 5  # Early stopping patience\n",
        "\n",
        "    # Training parameters\n",
        "    BATCH_SIZE = 64 # Should be in multiple of SAVE_BATCH_SIZE\n",
        "    NUM_EPOCHS = 10  # Small number for testing\n",
        "    LEARNING_RATE = 0.001\n",
        "    SAVE_BEST = True\n",
        "\n",
        "    # Other\n",
        "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    SAVE_DIR = '/content/checkpoints/'\n",
        "    NUM_WORKERS = 4\n",
        "\n",
        "    # Classes\n",
        "    CLASS_NAMES = ['NORM', 'MI', 'STTC', 'CD', 'HYP']\n",
        "    NUM_CLASSES = 5\n",
        "\n",
        "\n",
        "# Update save directory\n",
        "Config.SAVE_DIR = SAVE_DIR\n",
        "config = Config()\n",
        "print(f\"✓ Configuration loaded\")\n",
        "print(f\"  Device: {config.DEVICE}\")\n",
        "print(f\"  Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"  Epochs: {config.NUM_EPOCHS}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- Downloading the data and extracting in /data -->\n",
        "## From: Downloading and extracting PTB-CL dataset into /data"
      ],
      "metadata": {
        "id": "bDsHWvUlCTeT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "import tarfile\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# Create data directory\n",
        "os.makedirs('/content/data', exist_ok=True)\n",
        "os.chdir('/content/data')\n",
        "\n",
        "# ============================================================================\n",
        "# Download and Setup PTB-XL Dataset in Google Colab\n",
        "# ============================================================================\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"DOWNLOADING PTB-XL DATASET\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# ============================================================================\n",
        "# Method 1: Download from PhysioNet (Official Source)\n",
        "# ============================================================================\n",
        "\n",
        "def download_with_progress(url, filename):\n",
        "    \"\"\"Download file with progress bar\"\"\"\n",
        "    class DownloadProgressBar(tqdm):\n",
        "        def update_to(self, b=1, bsize=1, tsize=None):\n",
        "            if tsize is not None:\n",
        "                self.total = tsize\n",
        "            self.update(b * bsize - self.n)\n",
        "\n",
        "    with DownloadProgressBar(unit='B', unit_scale=True, miniters=1, desc=filename) as t:\n",
        "        urllib.request.urlretrieve(url, filename=filename, reporthook=t.update_to)\n",
        "\n",
        "# Download PTB-XL dataset (about 8.5 GB)\n",
        "print(\"\\n1. Downloading PTB-XL dataset from PhysioNet...\")\n",
        "print(\"   This may take 10-20 minutes depending on connection speed...\")\n",
        "\n",
        "PTB_XL_URL = \"https://physionet.org/static/published-projects/ptb-xl/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3.zip\"\n",
        "PTB_XL_ZIP = \"ptb-xl-dataset.zip\"\n",
        "\n",
        "download_with_progress(PTB_XL_URL, PTB_XL_ZIP)\n",
        "print(\"✓ Download complete!\")\n",
        "\n",
        "# ============================================================================\n",
        "# Extract the dataset\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n2. Extracting dataset...\")\n",
        "with zipfile.ZipFile(PTB_XL_ZIP, 'r') as zip_ref:\n",
        "    # Get list of all files\n",
        "    file_list = zip_ref.namelist()\n",
        "\n",
        "    # Extract with progress bar\n",
        "    for file in tqdm(file_list, desc=\"Extracting\"):\n",
        "        zip_ref.extract(file, '/content/data/')\n",
        "\n",
        "print(\"✓ Extraction complete!\")\n",
        "\n",
        "# Move files from nested directory to /content/data/\n",
        "print(\"\\n3. Organizing files...\")\n",
        "import shutil\n",
        "\n",
        "extracted_dir = '/content/data/ptb-xl-a-large-publicly-available-electrocardiography-dataset-1.0.3'\n",
        "if os.path.exists(extracted_dir):\n",
        "    # Move all contents to /content/data/\n",
        "    for item in os.listdir(extracted_dir):\n",
        "        src = os.path.join(extracted_dir, item)\n",
        "        dst = os.path.join('/content/data/', item)\n",
        "        if os.path.exists(dst):\n",
        "            if os.path.isdir(dst):\n",
        "                shutil.rmtree(dst)\n",
        "            else:\n",
        "                os.remove(dst)\n",
        "        shutil.move(src, dst)\n",
        "\n",
        "    # Remove empty extracted directory\n",
        "    shutil.rmtree(extracted_dir)\n",
        "    print(\"✓ Files organized!\")\n",
        "\n",
        "# Clean up zip file\n",
        "os.remove(PTB_XL_ZIP)\n",
        "print(\"✓ Cleaned up temporary files\")\n"
      ],
      "metadata": {
        "id": "_PpsivFlCuQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install rarfile\n",
        "\n",
        "from google.colab import drive\n",
        "import shutil\n",
        "import rarfile\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Option 1: If you have the dataset in your Drive\n",
        "# print(\"Copying Signals and Labels from Google Drive...\")\n",
        "SIGNALS_PATH = '/content/drive/MyDrive/signals.rar'\n",
        "LABELS_PATH = '/content/drive/MyDrive/labels.rar'\n",
        "\n",
        "rar_path_signals = '/content/data/signals.rar'\n",
        "rar_path_labels = '/content/data/labels.rar'\n",
        "extract_to = '/content/data/'\n",
        "\n",
        "if os.path.exists(SIGNALS_PATH):\n",
        "    shutil.copy(SIGNALS_PATH, '/content/data/signals.rar')\n",
        "if os.path.exists(LABELS_PATH):\n",
        "    shutil.copy(LABELS_PATH, '/content/data/labels.rar')\n",
        "\n",
        "# Extract\n",
        "print(\"Extracting Signals and Labels from into /data...\")\n",
        "with rarfile.RarFile(rar_path_signals) as rf:\n",
        "    rf.extractall(path=extract_to)\n",
        "with rarfile.RarFile(rar_path_labels) as rf:\n",
        "    rf.extractall(path=extract_to)\n",
        "\n",
        "print(\"✓ Dataset copied and extracted from Google Drive!\")"
      ],
      "metadata": {
        "id": "NNi1ohjpITx8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Verify the dataset structure\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\n4. Verifying dataset structure...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "required_files = [\n",
        "    'ptbxl_database.csv',\n",
        "    'scp_statements.csv',\n",
        "    'records100',  # 100Hz recordings\n",
        "    'records500',  # 500Hz recordings\n",
        "]\n",
        "\n",
        "all_present = True\n",
        "for item in required_files:\n",
        "    path = os.path.join('/content/data/', item)\n",
        "    if os.path.exists(path):\n",
        "        if os.path.isdir(path):\n",
        "            num_files = len([f for f in os.listdir(path) if not f.startswith('.')])\n",
        "            print(f\"✓ {item}/ - {num_files} files\")\n",
        "        else:\n",
        "            size_mb = os.path.getsize(path) / (1024 * 1024)\n",
        "            print(f\"✓ {item} - {size_mb:.2f} MB\")\n",
        "    else:\n",
        "        print(f\"✗ {item} - NOT FOUND\")\n",
        "        all_present = False\n",
        "\n",
        "if all_present:\n",
        "    print(\"\\n✓ Dataset verification successful!\")\n",
        "    print(\"=\" * 80)\n",
        "else:\n",
        "    print(\"\\n⚠ Some files are missing!\")\n",
        "    print(\"=\" * 80)"
      ],
      "metadata": {
        "id": "VbIs4Wp9DGym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================================================\n",
        "# Display dataset info\n",
        "# ============================================================================\n",
        "\n",
        "print(\"\\nDATASET INFORMATION:\")\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/data/ptbxl_database.csv')\n",
        "print(f\"Total ECG records: {len(df)}\")\n",
        "print(f\"Sampling rates: 100Hz and 500Hz\")\n",
        "print(f\"Duration: 10 seconds\")\n",
        "print(f\"Leads: 12-lead ECG\")\n",
        "\n",
        "print(\"\\nClass distribution (diagnostic superclass):\")\n",
        "df['scp_codes'] = df['scp_codes'].apply(lambda x: eval(x))\n",
        "\n",
        "classes = ['NORM', 'MI', 'STTC', 'CD', 'HYP']\n",
        "for cls in classes:\n",
        "    count = sum(cls in codes for codes in df['scp_codes'])\n",
        "    percentage = (count / len(df)) * 100\n",
        "    print(f\"  {cls}: {count} ({percentage:.1f}%)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"✓ PTB-XL DATASET READY!\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\nDataset location: /content/data/\")\n",
        "print(f\"Database file: /content/data/ptbxl_database.csv\")\n",
        "print(f\"100Hz signals: /content/data/records100/\")\n",
        "print(f\"500Hz signals: /content/data/records500/\")\n",
        "print(\"\\nYou can now proceed to preprocess the data!\")"
      ],
      "metadata": {
        "id": "M9YagsLlDOj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bomI4JU8iRv"
      },
      "source": [
        "## 3. Data Loading - Precision Cache\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRSdtfzD8iRv"
      },
      "source": [
        "**From: `precission_cache.py`** - Your memory-efficient caching system\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xVh3DVy8iRv"
      },
      "outputs": [],
      "source": [
        "from collections import OrderedDict\n",
        "\n",
        "class PrecisionCache:\n",
        "    def __init__(self, capacity: int):\n",
        "        \"\"\"\n",
        "        capacity: Maximum number of batches to store in RAM.\n",
        "        \"\"\"\n",
        "        self.cache = OrderedDict()\n",
        "        self.capacity = capacity\n",
        "\n",
        "    def get_batch(self, key):\n",
        "        \"\"\"Returns the batch if it exists, otherwise returns None.\"\"\"\n",
        "        if key not in self.cache:\n",
        "            return None\n",
        "\n",
        "        # Move to end (mark as most recently used)\n",
        "        self.cache.move_to_end(key)\n",
        "        return self.cache[key]\n",
        "\n",
        "    def add_batch(self, key, batch_data):\n",
        "        \"\"\"Adds a batch. Evicts the oldest if capacity is reached.\"\"\"\n",
        "        if key in self.cache:\n",
        "            self.cache.move_to_end(key)\n",
        "        self.cache[key] = batch_data\n",
        "\n",
        "        if len(self.cache) > self.capacity:\n",
        "            # last=False pops the first item (the Least Recently Used)\n",
        "            self.cache.popitem(last=False)\n",
        "\n",
        "print(\"✓ PrecisionCache loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWfPZzbt8iRw"
      },
      "source": [
        "## 4. Data Loading - Signal DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OEK9mBr8iRw"
      },
      "source": [
        "**From: `signal_dataloader.py`** - Your signal data loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9YTcwsC8iRw",
        "outputId": "e01b3c6b-2c40-46a7-d1d7-1bbee4d6380f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ SignalDataLoader loaded\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "# from .precission_cache import PrecisionCache\n",
        "\n",
        "class ECGSignalDataset(Dataset):\n",
        "    \"\"\"PyTorch Dataset for PTB-XL ECG data\"\"\"\n",
        "\n",
        "    def __init__(self, signals_path, labels_path, saved_batch_size, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            signals: Path to directory containing image batches (e.g., 'data/signals/train/')\n",
        "            labels_path: Path to labels .npy file\n",
        "            saved_batch_size: Number of samples per saved batch file (default: 32)\n",
        "            transform: Optional transforms to apply\n",
        "        \"\"\"\n",
        "        # data can be either raw signals or precomputed images depending on the model\n",
        "        self.image_path = signals_path\n",
        "        self.labels = np.load(labels_path)\n",
        "        self.saved_batch_size = saved_batch_size\n",
        "        self.transform = transform\n",
        "        self.cache = PrecisionCache(capacity=50)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Calculate which batch file and position within that batch\n",
        "        batch_no = idx // self.saved_batch_size\n",
        "        sample_idx = idx % self.saved_batch_size\n",
        "\n",
        "        # Try to get from cache\n",
        "        batch = self.cache.get_batch(batch_no)\n",
        "\n",
        "        # Load from disk if not in cache\n",
        "        if batch is None:\n",
        "            batch_file = os.path.join(self.image_path, f\"batch_{batch_no}.npy\")\n",
        "\n",
        "            if not os.path.exists(batch_file):\n",
        "                raise FileNotFoundError(f\"Batch file not found: {batch_file}\")\n",
        "\n",
        "            batch = np.load(batch_file)\n",
        "            self.cache.add_batch(batch_no, batch)\n",
        "\n",
        "        # Handle last batch edge case\n",
        "        if sample_idx >= len(batch):\n",
        "            raise IndexError(\n",
        "                f\"Sample index {sample_idx} out of bounds for batch {batch_no} \"\n",
        "                f\"with {len(batch)} samples\"\n",
        "            )\n",
        "\n",
        "        # Get image and label\n",
        "        signal = batch[sample_idx].astype(np.float32)\n",
        "        label = self.labels[idx].astype(np.float32)\n",
        "\n",
        "        # Convert to tensors\n",
        "        signal = torch.from_numpy(signal).float()\n",
        "        label = torch.from_numpy(label).float()\n",
        "\n",
        "        # Apply transforms if any\n",
        "        if self.transform:\n",
        "            signal = self.transform(signal)\n",
        "\n",
        "        return signal, label\n",
        "\n",
        "def create_signal_dataloader(\n",
        "    image_path,\n",
        "    labels_path,\n",
        "    saved_batch_size=32,\n",
        "    batch_size=16,  # DataLoader batch size (can be different from saved_batch_size)\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        "):\n",
        "    \"\"\"\n",
        "    Create dataloader for signal data\n",
        "\n",
        "    Args:\n",
        "        signals_path: Path to Signals directory\n",
        "        labels_path: Path to labels file\n",
        "        saved_batch_size: Size of saved batch files (default: 32)\n",
        "        batch_size: DataLoader batch size (default: 16)\n",
        "        shuffle: Whether to shuffle data\n",
        "        num_workers: Number of worker processes\n",
        "    \"\"\"\n",
        "    dataset = ECGSignalDataset(\n",
        "        signals_path=image_path,\n",
        "        labels_path=labels_path,\n",
        "        saved_batch_size=saved_batch_size,\n",
        "        transform=None\n",
        "    )\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory= torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "    return loader\n",
        "\n",
        "\n",
        "\n",
        "print(\"✓ SignalDataLoader loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CqK2kxAI8iRw"
      },
      "source": [
        "## 5. Data Loading - Image DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeR85Cux8iRw"
      },
      "source": [
        "**From: `image_dataloader.py`** - Your image data loader (GAF/MTF)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQfGUj2L8iRw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "DataLoader for ECG Images (GAF/MTF/MT or multiple)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "# from .precission_cache import PrecisionCache\n",
        "\n",
        "\n",
        "class ECGImageDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for loading batched ECG images\n",
        "    \"\"\"\n",
        "    def __init__(self, image_path, labels_path, saved_batch_size=32, transform_type='gaf', transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_path: Path to directory containing image batches (e.g., 'data/images/train/')\n",
        "            labels_path: Path to labels .npy file\n",
        "            saved_batch_size: Number of samples per saved batch file (default: 32)\n",
        "            transform_type: 'gaf', 'mtf', or ['gaf', 'mtf'] to load both\n",
        "            transform: Optional transforms to apply\n",
        "        \"\"\"\n",
        "        self.labels = np.load(labels_path)\n",
        "        self.saved_batch_size = saved_batch_size\n",
        "        self.transform = transform\n",
        "        self.cache = PrecisionCache(capacity=50)\n",
        "\n",
        "        # Handle both string and list input for transform_type\n",
        "        if isinstance(transform_type, str):\n",
        "            self.transform_type = [transform_type]\n",
        "        else:\n",
        "            self.transform_type = transform_type\n",
        "\n",
        "        # Build image paths for each transform type\n",
        "        self.image_paths = []\n",
        "        for t_type in self.transform_type:\n",
        "            img_path = os.path.join(image_path, t_type)\n",
        "            if not os.path.exists(img_path):\n",
        "                raise ValueError(f\"Image directory not found: {img_path}\")\n",
        "            self.image_paths.append(img_path)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Calculate which batch file and position within that batch\n",
        "        batch_no = idx // self.saved_batch_size\n",
        "        sample_idx = idx % self.saved_batch_size\n",
        "\n",
        "        # Get image(s) from all transform types\n",
        "        images = []\n",
        "        for i, image_path in enumerate(self.image_paths):\n",
        "            # Try to get from cache (use unique key combining path index and batch number)\n",
        "            cache_key = (i, batch_no)\n",
        "            batch = self.cache.get_batch(cache_key)\n",
        "\n",
        "            # Load from disk if not in cache\n",
        "            if batch is None:\n",
        "                batch_file = os.path.join(image_path, f\"batch_{batch_no}.npy\")\n",
        "\n",
        "                if not os.path.exists(batch_file):\n",
        "                    raise FileNotFoundError(f\"Batch file not found: {batch_file}\")\n",
        "\n",
        "                batch = np.load(batch_file)\n",
        "                self.cache.add_batch(cache_key, batch)\n",
        "\n",
        "            # Handle last batch edge case\n",
        "            if sample_idx >= len(batch):\n",
        "                raise IndexError(\n",
        "                    f\"Sample index {sample_idx} out of bounds for batch {batch_no} \"\n",
        "                    f\"with {len(batch)} samples\"\n",
        "                )\n",
        "\n",
        "            # Get image\n",
        "            image = batch[sample_idx].astype(np.float32)\n",
        "\n",
        "            # Normalize image to [0, 1] if needed\n",
        "            if image.max() > 1.0:\n",
        "                image = image / 255.0\n",
        "\n",
        "            images.append(image)\n",
        "\n",
        "        # Get label\n",
        "        label = self.labels[idx].astype(np.float32)\n",
        "\n",
        "        # Convert to tensors and apply transforms\n",
        "        if len(images) > 1:\n",
        "            # Return separate tensors for each image type\n",
        "            image_tensors = []\n",
        "            for image in images:\n",
        "                img_tensor = torch.from_numpy(image).float()\n",
        "                if self.transform:\n",
        "                    img_tensor = self.transform(img_tensor)\n",
        "                image_tensors.append(img_tensor)\n",
        "            label = torch.from_numpy(label).float()\n",
        "            return tuple(image_tensors), label\n",
        "        else:\n",
        "            # Return single image tensor\n",
        "            image = torch.from_numpy(images[0]).float()\n",
        "            label = torch.from_numpy(label).float()\n",
        "            if self.transform:\n",
        "                image = self.transform(image)\n",
        "            return image, label\n",
        "\n",
        "\n",
        "def create_image_dataloader(\n",
        "    image_path,\n",
        "    labels_path,\n",
        "    saved_batch_size=32,\n",
        "    transform_type='gaf',\n",
        "    batch_size=16,  # DataLoader batch size (can be different from saved_batch_size)\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        "):\n",
        "    \"\"\"\n",
        "    Create dataloader for image data\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to image directory\n",
        "        labels_path: Path to labels file\n",
        "        saved_batch_size: Size of saved batch files (default: 32)\n",
        "        transform_type: 'gaf', 'mtf', or ['gaf', 'mtf'] to load both types\n",
        "        batch_size: DataLoader batch size (default: 16)\n",
        "        shuffle: Whether to shuffle data\n",
        "        num_workers: Number of worker processes\n",
        "    \"\"\"\n",
        "    dataset = ECGImageDataset(\n",
        "        image_path=image_path,\n",
        "        labels_path=labels_path,\n",
        "        saved_batch_size=saved_batch_size,\n",
        "        transform_type=transform_type,\n",
        "        transform=None\n",
        "    )\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory= torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "    return loader\n",
        "\n",
        "\n",
        "\n",
        "print(\"✓ ImageDataLoader loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CMbDsEK-8iRw"
      },
      "source": [
        "## 6. Data Loading - Fusion DataLoader\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lCmlQPv_8iRw"
      },
      "source": [
        "**From: `fusion_dataloader.py`** - Your fusion data loader\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-NH0Eyki8iRw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "DataLoader for ECG Images (GAF/MTF)\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import os\n",
        "# from .precission_cache import PrecisionCache\n",
        "\n",
        "\n",
        "class ECGFusionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset for loading batched ECG Signalss + images\n",
        "    \"\"\"\n",
        "    def __init__(self, signal_path, image_path, labels_path, saved_batch_size=32, transform_type='gaf', transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            image_path: Path to directory containing image batches (e.g., 'data/images/train/')\n",
        "            signal_path: Path to directory containing signals batches (e.g., 'data/signals/train/')\n",
        "            labels_path: Path to labels .npy file\n",
        "            saved_batch_size: Number of samples per saved batch file (default: 32)\n",
        "            transform_type: 'gaf' or 'mtf'\n",
        "            transform: Optional transforms to apply\n",
        "        \"\"\"\n",
        "        self.image_path = os.path.join(image_path, transform_type)\n",
        "        self.signal_path = os.path.join(signal_path)\n",
        "        self.labels = np.load(labels_path)\n",
        "        self.saved_batch_size = saved_batch_size\n",
        "        self.transform_type = transform_type\n",
        "        self.transform = transform\n",
        "        self.cache = PrecisionCache(capacity=50)\n",
        "\n",
        "        # Verify directories exists\n",
        "        if not os.path.exists(self.image_path):\n",
        "            raise ValueError(f\"Image directory not found: {self.image_path}\")\n",
        "        if not os.path.exists(self.signal_path):\n",
        "            raise ValueError(f\"Signal directory not found: {self.signal_path}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Calculate which batch file and position within that batch\n",
        "        batch_no = idx // self.saved_batch_size\n",
        "        sample_idx = idx % self.saved_batch_size\n",
        "\n",
        "        # Try to get from cache\n",
        "        signal_batch = self.cache.get_batch(f\"signal_b_{batch_no}\")\n",
        "        image_batch = self.cache.get_batch(f\"image_b_{batch_no}\")\n",
        "\n",
        "        # Load from disk if not in cache\n",
        "        if signal_batch is None:\n",
        "            batch_file = os.path.join(self.signal_path, f\"batch_{batch_no}.npy\")\n",
        "\n",
        "            if not os.path.exists(batch_file):\n",
        "                raise FileNotFoundError(f\"Signals Batch file not found: {batch_file}\")\n",
        "\n",
        "            signal_batch = np.load(batch_file)\n",
        "            self.cache.add_batch(f\"signal_b_{batch_no}\", signal_batch)\n",
        "\n",
        "        if image_batch is None:\n",
        "            batch_file = os.path.join(self.image_path, f\"batch_{batch_no}.npy\")\n",
        "\n",
        "            if not os.path.exists(batch_file):\n",
        "                raise FileNotFoundError(f\"Image Batch file not found: {batch_file}\")\n",
        "\n",
        "            image_batch = np.load(batch_file)\n",
        "            self.cache.add_batch(f\"image_b_{batch_no}\", image_batch)\n",
        "\n",
        "        # Handle last batch edge case\n",
        "        if sample_idx >= len(signal_batch) or sample_idx >= len(image_batch):\n",
        "            raise IndexError(\n",
        "                f\"Sample index {sample_idx} out of bounds for batch {batch_no} \"\n",
        "                f\"with {len(signal_batch)} samples\"\n",
        "            )\n",
        "        # Get image and label\n",
        "        signal = signal_batch[sample_idx].astype(np.float32)\n",
        "        image = image_batch[sample_idx].astype(np.float32)\n",
        "        label = self.labels[idx].astype(np.float32)\n",
        "\n",
        "        # Normalize image to [0, 1] if needed\n",
        "        if image.max() > 1.0:\n",
        "            image = image / 255.0\n",
        "\n",
        "        # Convert to tensors\n",
        "        signal = torch.from_numpy(signal).float()\n",
        "        image = torch.from_numpy(image).float()\n",
        "        label = torch.from_numpy(label).float()\n",
        "\n",
        "        # Apply transforms if any\n",
        "        if self.transform:\n",
        "            signal = self.transform(signal)\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return (signal, image), label\n",
        "\n",
        "\n",
        "def create_fusion_dataloader(\n",
        "    signal_path,\n",
        "    image_path,\n",
        "    labels_path,\n",
        "    saved_batch_size=32,\n",
        "    transform_type='gaf',\n",
        "    batch_size=32,  # DataLoader batch size (can be different from saved_batch_size)\n",
        "    shuffle=True,\n",
        "    num_workers=4\n",
        "):\n",
        "    \"\"\"\n",
        "    Create dataloader for image data\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to image directory\n",
        "        labels_path: Path to labels file\n",
        "        saved_batch_size: Size of saved batch files (default: 32)\n",
        "        transform_type: 'gaf' or 'mtf'\n",
        "        batch_size: DataLoader batch size (default: 16)\n",
        "        shuffle: Whether to shuffle data\n",
        "        num_workers: Number of worker processes\n",
        "    \"\"\"\n",
        "    dataset = ECGFusionDataset(\n",
        "        image_path=image_path,\n",
        "        signal_path= signal_path,\n",
        "        labels_path=labels_path,\n",
        "        saved_batch_size=saved_batch_size,\n",
        "        transform_type=transform_type,\n",
        "        transform=None\n",
        "    )\n",
        "\n",
        "    loader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory= torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "    return loader\n",
        "\n",
        "\n",
        "\n",
        "print(\"✓ FusionDataLoader loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTW0Vcym8iRw"
      },
      "source": [
        "## 7. Models - RNN Block\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JollDlOm8iRw"
      },
      "source": [
        "**From: `rnn_block.py`** - Your RNN building blocks (GRU/LSTM)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q-vKSoWb8iRw"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "RNN Models for ECG Classification\n",
        "- GRU\n",
        "- BiGRU\n",
        "- LSTM\n",
        "- BiLSTM\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class RNN_BLOCK(nn.Module):\n",
        "    \"\"\"Base class for GRU/LSTM models\"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=3,       # 3 leads\n",
        "        hidden_size=128,\n",
        "        num_layers=2,\n",
        "        dropout=0.3,\n",
        "        bidirectional=False,\n",
        "        rnn_type='gru'      # 'gru' or 'lstm'\n",
        "    ):\n",
        "        super(RNN_BLOCK, self).__init__()\n",
        "\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.bidirectional = bidirectional\n",
        "        self.rnn_type = rnn_type\n",
        "        self.d = 2 if self.bidirectional else 1\n",
        "\n",
        "        if self.rnn_type == 'gru':\n",
        "            self.rnn = nn.GRU(\n",
        "                input_size=self.input_size,\n",
        "                hidden_size=self.hidden_size,\n",
        "                num_layers=self.num_layers,\n",
        "                dropout=dropout if self.num_layers > 1 else 0,\n",
        "                bidirectional=self.bidirectional,\n",
        "                batch_first=True\n",
        "            )\n",
        "        elif self.rnn_type == 'lstm':\n",
        "            self.rnn = nn.LSTM(\n",
        "                input_size=self.input_size,\n",
        "                hidden_size=self.hidden_size,\n",
        "                num_layers=self.num_layers,\n",
        "                dropout=dropout if self.num_layers > 1 else 0,\n",
        "                bidirectional=self.bidirectional,\n",
        "                batch_first=True,\n",
        "            )\n",
        "        else:\n",
        "            raise ValueError(f\"Unidentified rnn type: {rnn_type}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, num_leads, signal_length) = (batch, 3, 1000)\n",
        "        Returns:\n",
        "            out: (batch, seq_length, hidden_size * d)\n",
        "            hidden: (batch, hidden_size * d)\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "\n",
        "        # Transpose to (batch, seq_len, features)\n",
        "        x_transposed = x.transpose(1, 2)  # (batch, 1000, 3)\n",
        "\n",
        "        # Initialize hidden state\n",
        "        h0 = torch.zeros(self.num_layers * self.d, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        if self.rnn_type == 'gru':\n",
        "            out, hn = self.rnn(x_transposed, h0)\n",
        "        elif self.rnn_type == 'lstm':\n",
        "            c0 = torch.zeros(self.num_layers * self.d, x.size(0), self.hidden_size).to(device)\n",
        "            out, (hn, cn) = self.rnn(x_transposed, (h0, c0))\n",
        "\n",
        "        # Extract hidden state\n",
        "        if not self.bidirectional:\n",
        "            hidden = hn[-1]  # (batch, hidden_size)\n",
        "        else:\n",
        "            # Concatenate forward and backward hidden states\n",
        "            hidden = torch.cat([hn[-2], hn[-1]], dim=1)  # (batch, hidden_size*2)\n",
        "\n",
        "        return out, hidden\n",
        "\n",
        "\n",
        "class FullyConnected(nn.Module):\n",
        "    \"\"\"Fully connected layers for classification\"\"\"\n",
        "    def __init__(self, input_size, output_size, dropout=0.3):\n",
        "        super(FullyConnected, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(input_size, input_size * 2)\n",
        "        self.bn1 = nn.BatchNorm1d(input_size * 2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "\n",
        "        self.fc2 = nn.Linear(input_size * 2, input_size)\n",
        "        self.bn2 = nn.BatchNorm1d(input_size)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        self.output = nn.Linear(input_size, output_size)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, input_size)\n",
        "        Returns:\n",
        "            logits: (batch, output_size)\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "\n",
        "        out = self.fc1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.dropout1(out)\n",
        "\n",
        "        out = self.fc2(out)\n",
        "        out = self.bn2(out)\n",
        "        out = F.relu(out)\n",
        "        out = self.dropout2(out)\n",
        "\n",
        "        logits = self.output(out)\n",
        "        return logits\n",
        "\n",
        "print(\"✓ RNN_BLOCK and FullyConnected loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wWSz37ts8iRx"
      },
      "source": [
        "## 8. Models - RNN Attention Mechanisms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPZXSJk-8iRx"
      },
      "source": [
        "**From: `rnn_attention.py`** - Your attention mechanisms for RNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KMTsqSnI8iRx"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Attention mechanisms for ECG classification\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def scaled_dot_product_attention(Q, K, V, scale):\n",
        "    \"\"\"\n",
        "    Scaled dot-product attention mechanism\n",
        "\n",
        "    Args:\n",
        "        Q: Query (batch, seq_len, hidden_size)\n",
        "        K: Key (batch, seq_len, hidden_size)\n",
        "        V: Value (batch, seq_len, hidden_size)\n",
        "        scale: Scaling factor (usually sqrt(hidden_size))\n",
        "\n",
        "    Returns:\n",
        "        output: (batch, seq_len, hidden_size)\n",
        "        weights: (batch, seq_len, seq_len)\n",
        "    \"\"\"\n",
        "    scores = torch.bmm(Q, K.transpose(1, 2)) / scale  # (batch, seq_len, seq_len)\n",
        "    weights = F.softmax(scores, dim=-1)\n",
        "    output = torch.bmm(weights, V)  # (batch, seq_len, hidden_size)\n",
        "    return output, weights\n",
        "\n",
        "\n",
        "class AdditiveAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Additive attention (Bahdanau attention)\n",
        "    Computes attention weights over sequence and returns weighted sum\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super(AdditiveAttention, self).__init__()\n",
        "        self.attention = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_len, hidden_size)\n",
        "        Returns:\n",
        "            context: (batch, hidden_size)\n",
        "            attention_weights: (batch, seq_len)\n",
        "        \"\"\"\n",
        "        # Compute attention scores\n",
        "        attention_scores = self.attention(x)  # (batch, seq_len, 1)\n",
        "        attention_scores = attention_scores.squeeze(-1)  # (batch, seq_len)\n",
        "\n",
        "        # Compute attention weights\n",
        "        attention_weights = F.softmax(attention_scores, dim=1)  # (batch, seq_len)\n",
        "\n",
        "        # Compute context vector (weighted sum)\n",
        "        context = torch.bmm(\n",
        "            attention_weights.unsqueeze(1),  # (batch, 1, seq_len)\n",
        "            x  # (batch, seq_len, hidden_size)\n",
        "        )  # (batch, 1, hidden_size)\n",
        "        context = context.squeeze(1)  # (batch, hidden_size)\n",
        "\n",
        "        return context, attention_weights\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Self-attention mechanism\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        super(SelfAttention, self).__init__()\n",
        "\n",
        "        self.query = nn.Linear(hidden_size, hidden_size)\n",
        "        self.key = nn.Linear(hidden_size, hidden_size)\n",
        "        self.value = nn.Linear(hidden_size, hidden_size)\n",
        "        self.scale = hidden_size ** 0.5\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_len, hidden_size)\n",
        "        Returns:\n",
        "            attention: (batch, seq_len, hidden_size)\n",
        "            weights: (batch, seq_len, seq_len)\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "        self.device = device\n",
        "\n",
        "        Q = self.query(x)\n",
        "        K = self.key(x)\n",
        "        V = self.value(x)\n",
        "\n",
        "        attention, weights = scaled_dot_product_attention(Q, K, V, self.scale)\n",
        "        return attention, weights\n",
        "\n",
        "\n",
        "class CrossAttention:\n",
        "    \"\"\"\n",
        "    Note: This is private and experimental.\n",
        "    Neither allowed to embed into training, nor allowed to replicate.\n",
        "    Copyright protected.\n",
        "    Author: Sujit Patel.\n",
        "    https://github.com/sujitpatel2739/fusion_ecg\n",
        "    \"\"\"\n",
        "    def __init__(self, hidden_size):\n",
        "        self.query = nn.Linear(hidden_size, hidden_size)\n",
        "        self.key = nn.Linear(hidden_size, hidden_size)\n",
        "        self.value = nn.Linear(hidden_size, hidden_size)\n",
        "        self.scale = hidden_size ** 0.5\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def forward(self, x, context):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, seq_len_x, hidden_size) - query source\n",
        "            context: (batch, seq_len_c, hidden_size) - key/value source\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "        self.device = device\n",
        "\n",
        "        Q = self.query(x)\n",
        "        K = self.key(context)\n",
        "        V = self.value(context)\n",
        "\n",
        "        attention, weights = scaled_dot_product_attention(Q, K, V, self.scale)\n",
        "        return attention, weights\n",
        "\n",
        "print(\"✓ Attention mechanisms loaded (Additive, Self)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-bhwGY48iRx"
      },
      "source": [
        "## 9. Models - Complete RNN Model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD9Xczzq8iRx"
      },
      "source": [
        "**From: `rnn.py`** - Your complete RNN model with convenience functions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cxklb7Aa8iRx"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Complete ECG Classification Model\n",
        "Combines RNN block + optional Attention + FC layers\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "# from .rnn_block import RNN_BLOCK, FullyConnected\n",
        "# from .rnn_attention import AdditiveAttention, SelfAttention\n",
        "\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Configurable ECG classification model\n",
        "    Supports: GRU, BiGRU, LSTM, BiLSTM with optional Attention\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_size=3,\n",
        "        hidden_size=128,\n",
        "        num_layers=2,\n",
        "        num_classes=5,\n",
        "        dropout=0.3,\n",
        "        bidirectional=False,\n",
        "        attention=None,  # None, 'additive', or 'self'\n",
        "        rnn_type='gru'   # 'gru' or 'lstm'\n",
        "    ):\n",
        "        super(RNNModel, self).__init__()\n",
        "\n",
        "        self.bidirectional = bidirectional\n",
        "        self.attention_type = attention\n",
        "        self.hidden_size = hidden_size\n",
        "        self.d = 2 if self.bidirectional else 1\n",
        "\n",
        "        self.rnn_block = RNN_BLOCK(\n",
        "            input_size=input_size,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_layers,\n",
        "            dropout=dropout,\n",
        "            bidirectional=bidirectional,\n",
        "            rnn_type=rnn_type\n",
        "        )\n",
        "\n",
        "        # Attention layer\n",
        "        if attention == 'additive':\n",
        "            self.attention = AdditiveAttention(hidden_size=hidden_size * self.d)\n",
        "        elif attention == 'self':\n",
        "            self.attention = SelfAttention(hidden_size=hidden_size * self.d)\n",
        "        else:\n",
        "            self.attention = None\n",
        "\n",
        "        # Fully connected layers\n",
        "        self.fc = FullyConnected(\n",
        "            input_size=hidden_size * self.d,\n",
        "            output_size=num_classes,\n",
        "            dropout=dropout\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, num_leads, signal_length) = (batch, 3, 1000)\n",
        "        Returns:\n",
        "            logits: (batch, num_classes)\n",
        "            attention_weights: (batch, seq_len) if attention, else None\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "        # RNN forward pass\n",
        "        rnn_out, hidden = self.rnn_block(x)\n",
        "        # rnn_out: (batch, seq_len, hidden_size*d)\n",
        "        # hidden: (batch, hidden_size*d)\n",
        "\n",
        "\n",
        "        # Apply attention if specified\n",
        "        if self.attention_type == 'additive':\n",
        "            context, attention_weights = self.attention(rnn_out)\n",
        "            # context: (batch, hidden_size*d)\n",
        "            fc_input = context\n",
        "\n",
        "        elif self.attention_type == 'self':\n",
        "            attention, attention_weights = self.attention(rnn_out)\n",
        "            # attention: (batch, seq_len, hidden_size*d)\n",
        "            # Use mean pooling over sequence\n",
        "            fc_input = attention.mean(dim=1)  # (batch, hidden_size*d)\n",
        "\n",
        "        else:\n",
        "            # No attention - use hidden state from RNN\n",
        "            attention_weights = None\n",
        "            fc_input = hidden\n",
        "\n",
        "        # Fully connected layers\n",
        "        logits = self.fc(fc_input)\n",
        "\n",
        "        if attention_weights is not None:\n",
        "            return (logits, attention_weights)\n",
        "        else:\n",
        "            return logits\n",
        "\n",
        "\n",
        "# ============= Convenience Functions =================================================\n",
        "\n",
        "def create_gru(hidden_size=128, num_layers=2, dropout=0.3):\n",
        "    \"\"\"Create unidirectional GRU model\"\"\"\n",
        "    return RNNModel(\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout,\n",
        "        bidirectional=False,\n",
        "        attention=None,\n",
        "        rnn_type='gru'\n",
        "    )\n",
        "\n",
        "\n",
        "def create_bigru(hidden_size=128, num_layers=2, dropout=0.3):\n",
        "    \"\"\"Create bidirectional GRU model\"\"\"\n",
        "    return RNNModel(\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout,\n",
        "        bidirectional=True,\n",
        "        attention=None,\n",
        "        rnn_type='gru'\n",
        "    )\n",
        "\n",
        "\n",
        "def create_lstm(hidden_size=128, num_layers=2, dropout=0.3):\n",
        "    \"\"\"Create unidirectional LSTM model\"\"\"\n",
        "    return RNNModel(\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout,\n",
        "        bidirectional=False,\n",
        "        attention=None,\n",
        "        rnn_type='lstm'\n",
        "    )\n",
        "\n",
        "\n",
        "def create_bilstm(hidden_size=128, num_layers=2, dropout=0.3):\n",
        "    \"\"\"Create bidirectional LSTM model\"\"\"\n",
        "    return RNNModel(\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout,\n",
        "        bidirectional=True,\n",
        "        attention=None,\n",
        "        rnn_type='lstm'\n",
        "    )\n",
        "\n",
        "\n",
        "def create_bigru_attention(hidden_size=128, num_layers=2, dropout=0.3, attention_type='additive'):\n",
        "    \"\"\"Create bidirectional GRU with attention\"\"\"\n",
        "    return RNNModel(\n",
        "        hidden_size=hidden_size,\n",
        "        num_layers=num_layers,\n",
        "        dropout=dropout,\n",
        "        bidirectional=True,\n",
        "        attention=attention_type,  # 'additive' or 'self'\n",
        "        rnn_type='gru'\n",
        "    )\n",
        "\n",
        "print(\"✓ RNN Model loaded\")\n",
        "print(\"  Available: create_gru, create_bigru, create_lstm, create_bilstm, create_bigru_attention\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syfd42yn8iRx"
      },
      "source": [
        "## 10. Models - CNN Attention Mechanisms\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GchdP4g48iRx"
      },
      "source": [
        "**From: `cnn_attention.py`** - Your CNN attention (Channel, Spatial, CBAM, Self)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2JedRw4p8iRx"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class ChannelAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Channel attention module (from CBAM paper)\n",
        "    Focuses on 'WHAT' is meaningful\n",
        "    \"\"\"\n",
        "    def __init__(self, num_channels, reduction_ratio=16):\n",
        "        super(ChannelAttention, self).__init__()\n",
        "\n",
        "        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
        "        self.max_pool = nn.AdaptiveMaxPool2d(1)\n",
        "\n",
        "        # Shared MLP\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(num_channels, num_channels // reduction_ratio, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(num_channels // reduction_ratio, num_channels, bias=False)\n",
        "        )\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.device =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, channels, height, width)\n",
        "        Returns:\n",
        "            out: (batch, channels, height, width)\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "        self.device = device\n",
        "\n",
        "        batch, channels, _, _ = x.size()\n",
        "\n",
        "        # Average pooling\n",
        "        avg_out = self.avg_pool(x).view(batch, channels)\n",
        "        avg_out = self.fc(avg_out)\n",
        "\n",
        "        # Max pooling\n",
        "        max_out = self.max_pool(x).view(batch, channels)\n",
        "        max_out = self.fc(max_out)\n",
        "\n",
        "        # Combine and apply sigmoid\n",
        "        attention = self.sigmoid(avg_out + max_out)\n",
        "        attention = attention.view(batch, channels, 1, 1)\n",
        "\n",
        "        return x * attention.expand_as(x)\n",
        "\n",
        "class SpatialAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Spatial attention module (from CBAM paper)\n",
        "    Focuses on 'WHERE' is meaningful\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, kernel_size=5):\n",
        "        super(SpatialAttention, self).__init__()\n",
        "\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        self.conv = nn.Conv2d(2, 1, kernel_size=kernel_size, padding=padding, bias=False)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.device =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, channels, height, width)\n",
        "        Returns:\n",
        "            out: (batch, channels, height, width)\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "        self.device = device\n",
        "\n",
        "        # Channel-wise pooling\n",
        "        avg_out = torch.mean(x, dim=1, keepdim=True)\n",
        "        max_out, _ = torch.max(x, dim=1, keepdim=True)\n",
        "\n",
        "        combined = torch.cat([avg_out, max_out], dim=1)\n",
        "\n",
        "        attention = self.conv(combined)\n",
        "        attention = self.sigmoid(attention)\n",
        "\n",
        "        return x * attention\n",
        "\n",
        "\n",
        "class SelfAttentionConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Self-Attention using 1x1 convolutions (Non-Local block)\n",
        "    Works on 2D feature maps from CNNs\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, reduction_ratio=8):\n",
        "        super(SelfAttentionConv, self).__init__()\n",
        "\n",
        "        # Reduce channels for spatial learning\n",
        "        self.inter_channels = in_channels // reduction_ratio\n",
        "\n",
        "        # Q, K, V using 1x1 convolutions\n",
        "        self.query_conv = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)\n",
        "        self.key_conv = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)\n",
        "\n",
        "        # Output projection, exapnding back the reduced channels\n",
        "        self.out_conv = nn.Conv2d(self.inter_channels, in_channels, kernel_size=1)\n",
        "\n",
        "        # Learnable scaling parameter (optional)\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.device =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, channels, height, width)\n",
        "        Returns:\n",
        "            out: (batch, channels, height, width)\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "        self.device = device\n",
        "\n",
        "        batch, channels, height, width = x.size()\n",
        "\n",
        "        Q = self.query_conv(x)   # (batch, inter_channels, H, W)\n",
        "        K = self.key_conv(x)     # (batch, inter_channels, H, W)\n",
        "        V = self.value_conv(x)   # (batch, inter_channels, H, W)\n",
        "\n",
        "        # Reshape for matrix multiplication: (batch, inter_channels, H*W)\n",
        "        Q = Q.view(batch, self.inter_channels, -1)\n",
        "        K = K.view(batch, self.inter_channels, -1)\n",
        "        V = V.view(batch, self.inter_channels, -1)\n",
        "\n",
        "        # Transpose K for dot product\n",
        "        K = K.permute(0, 2, 1)  # (batch, H*W, inter_channels)\n",
        "\n",
        "        # Attention scores\n",
        "        attention = torch.bmm(K, Q)  # (batch, H*W, H*W)\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        attention = attention.permute(0, 2, 1) # (batch, H*W, H*W)\n",
        "        out = torch.bmm(V, attention)  # (batch, inter_channels, H*W)\n",
        "\n",
        "        # Reshape back to spatial dimensions\n",
        "        out = out.view(batch, self.inter_channels, height, width)\n",
        "\n",
        "        # Project back to original channels\n",
        "        out = self.out_conv(out)  # (batch, channels, H, W)\n",
        "\n",
        "        # Residual connection with learnable weight; optionally sacling by gamma factor\n",
        "        out = self.gamma * out + x\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "class CrossAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Note: This is private and experimental.\n",
        "    Neither allowed to embed into training, nor allowed to replicate.\n",
        "    Copyright protected.\n",
        "    Author: Sujit Patel.\n",
        "    https://github.com/sujitpatel2739/fusion_ecg\n",
        "\n",
        "    Cross-Attention between GAF and MTF images\n",
        "    GAF provides Query, MTF provides Key and Value\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, reduction_ratio=8):\n",
        "        super(CrossAttention, self).__init__()\n",
        "\n",
        "        self.inter_channels = in_channels // reduction_ratio\n",
        "\n",
        "        # Q from GAF\n",
        "        self.query_conv = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)\n",
        "\n",
        "        # K, V from MTF\n",
        "        self.key_conv = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)\n",
        "        self.value_conv = nn.Conv2d(in_channels, self.inter_channels, kernel_size=1)\n",
        "\n",
        "        self.out_conv = nn.Conv2d(self.inter_channels, in_channels, kernel_size=1)\n",
        "\n",
        "        self.gamma = nn.Parameter(torch.zeros(1))\n",
        "        self.device =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def forward(self, gaf_features, mtf_features):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            gaf_features: (batch, channels, H, W) - Features from GAF CNN\n",
        "            mtf_features: (batch, channels, H, W) - Features from MTF CNN\n",
        "        Returns:\n",
        "            out: (batch, channels, H, W) - Attended features\n",
        "        \"\"\"\n",
        "        device = gaf_features.device\n",
        "        self.device = device\n",
        "\n",
        "        batch, channels, height, width = gaf_features.size()\n",
        "\n",
        "        # Q from GAF, K & V from MTF\n",
        "        Q = self.query_conv(gaf_features)   # (batch, inter_channels, H, W)\n",
        "        K = self.key_conv(mtf_features)     # (batch, inter_channels, H, W)\n",
        "        V = self.value_conv(mtf_features)   # (batch, inter_channels, H, W)\n",
        "\n",
        "        # Reshape\n",
        "        Q = Q.view(batch, self.inter_channels, -1)\n",
        "        K = K.view(batch, self.inter_channels, -1)\n",
        "        V = V.view(batch, self.inter_channels, -1)\n",
        "\n",
        "        K = K.permute(0, 2, 1)  # (batch, H*W, inter_channels)\n",
        "\n",
        "        # Attention\n",
        "        attention = torch.bmm(K, Q)  # (batch, H*W, H*W)\n",
        "        attention = F.softmax(attention, dim=-1)\n",
        "\n",
        "        # Apply to values\n",
        "        out = torch.bmm(V, attention.permute(0, 2, 1))\n",
        "        out = out.view(batch, self.inter_channels, height, width)\n",
        "        out = self.out_conv(out)\n",
        "\n",
        "        # Residual with GAF features\n",
        "        out = self.gamma * out + gaf_features\n",
        "\n",
        "        return out\n",
        "\n",
        "print(\"✓ CNN Attention mechanisms loaded\")\n",
        "print(\"  Available: ChannelAttention, SpatialAttention, SelfAttentionConv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AQj0q9kR8iRx"
      },
      "source": [
        "## 11. Models - CNN2D (AlexNet-inspired)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VarrWrxx8iRx"
      },
      "source": [
        "**From: `cnn2d_alexnet.py`** - Your custom AlexNet with attention variants\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6GfnHRf8iRx"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "2D CNN Models for ECG Image Classification\n",
        "- CNN2D (AlexNet-inspired)\n",
        "- CNN2D with various attention mechanisms\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "# from .cnn_attention import ChannelAttention, SpatialAttention, SelfAttentionConv\n",
        "\n",
        "\n",
        "class CNN2DClassifier(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout):\n",
        "        super(CNN2DClassifier, self).__init__()\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(in_channels, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, out_channels),\n",
        "        )\n",
        "        self.device =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        self.device = device\n",
        "\n",
        "        return self.classifier(x)\n",
        "\n",
        "\n",
        "# ============= CNN2D (AlexNet-inspired) ==============================================\n",
        "class CNN2D(nn.Module):\n",
        "    \"\"\"\n",
        "    AlexNet-inspired architecture adapted for ECG images\n",
        "    Supports multiple attention mechanisms: channel, spatial, self, or combinations\n",
        "\n",
        "    Args:\n",
        "        in_channels: Number of input channels (default: 3 for RGB-like images)\n",
        "        base_filters: Base number of filters (default: 16 for small datasets)\n",
        "        num_classes: Number of output classes (default: 5 for ECG)\n",
        "        dropout: Dropout rate (default: 0.5)\n",
        "        attention_type: None, 'channel', 'spatial', 'cbam', or 'self'\n",
        "        reduction_ratio: Channel reduction ratio for attention (default: 16)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        in_channels=3,\n",
        "        base_filters=16,\n",
        "        num_classes=5,\n",
        "        dropout=0.5,\n",
        "        attention_type=None,  # None, 'channel', 'spatial', 'cbam', 'self'\n",
        "        reduction_ratio=16\n",
        "    ):\n",
        "        super(CNN2D, self).__init__()\n",
        "\n",
        "        self.attention_type = attention_type\n",
        "\n",
        "        # Calculate channel sizes\n",
        "        c1 = base_filters          # 16\n",
        "        c2 = base_filters * 3      # 48\n",
        "        c3 = base_filters * 6      # 96\n",
        "        c4 = base_filters * 4      # 64\n",
        "        c5 = base_filters * 4      # 64\n",
        "\n",
        "        # Feature extraction layers\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, c1, kernel_size=11, stride=4, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(c1, c2, kernel_size=5, padding=2),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(c2, c3, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Attention after conv3 (if specified)\n",
        "        self.attention = self._build_attention(c3, attention_type, reduction_ratio)\n",
        "\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(c3, c4, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(c4, c5, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
        "        )\n",
        "\n",
        "        # Adaptive pooling to fixed size\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
        "\n",
        "        self.classifier = CNN2DClassifier(c5*6*6, num_classes, dropout)\n",
        "\n",
        "        self.device =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "    def _build_attention(self, num_channels, attention_type, reduction_ratio):\n",
        "        \"\"\"\n",
        "        Build attention module based on type\n",
        "\n",
        "        Args:\n",
        "            num_channels: Number of channels at this layer\n",
        "            attention_type: Type of attention\n",
        "            reduction_ratio: Channel reduction ratio\n",
        "\n",
        "        Returns:\n",
        "            nn.Module or None\n",
        "        \"\"\"\n",
        "        if attention_type is None or attention_type == 'none':\n",
        "            return None\n",
        "\n",
        "        elif attention_type == 'channel':\n",
        "            return ChannelAttention(num_channels, reduction_ratio)\n",
        "\n",
        "        elif attention_type == 'spatial':\n",
        "            return SpatialAttention(num_channels, kernel_size=7)\n",
        "\n",
        "        elif attention_type == 'cbam':\n",
        "            # CBAM = Channel + Spatial attention\n",
        "            return nn.Sequential(\n",
        "                ChannelAttention(num_channels, reduction_ratio),\n",
        "                SpatialAttention(num_channels, kernel_size=7)\n",
        "            )\n",
        "\n",
        "        elif attention_type == 'self':\n",
        "            return SelfAttentionConv(num_channels, reduction_ratio)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown attention type: {attention_type}\")\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, 3, 224, 224)\n",
        "        Returns:\n",
        "            logits: (batch, num_classes)\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "        self.device = device\n",
        "\n",
        "        # Feature extraction\n",
        "        out = self.conv1(x)\n",
        "        out = self.conv2(out)\n",
        "        out = self.conv3(out)\n",
        "\n",
        "        # Attention\n",
        "        if self.attention is not None:\n",
        "            out = self.attention(out)\n",
        "\n",
        "        out = self.conv4(out)\n",
        "        out = self.conv5(out)\n",
        "\n",
        "        # Pooling\n",
        "        out = self.avgpool(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "        logits = self.classifier(out)\n",
        "        return logits\n",
        "\n",
        "\n",
        "# ============= Convenience Functions =================================================\n",
        "\n",
        "def create_alexnet(num_classes=5, dropout=0.5):\n",
        "    \"\"\"Create AlexNet without attention\"\"\"\n",
        "    return CNN2D(\n",
        "        in_channels=3,\n",
        "        base_filters=16,\n",
        "        num_classes=num_classes,\n",
        "        dropout=dropout,\n",
        "        attention_type=None\n",
        "    )\n",
        "\n",
        "\n",
        "def create_alexnet_channel_attention(num_classes=5, dropout=0.5):\n",
        "    \"\"\"Create AlexNet with channel attention\"\"\"\n",
        "    return CNN2D(\n",
        "        in_channels=3,\n",
        "        base_filters=16,\n",
        "        num_classes=num_classes,\n",
        "        dropout=dropout,\n",
        "        attention_type='channel'\n",
        "    )\n",
        "\n",
        "\n",
        "def create_alexnet_spatial_attention(num_classes=5, dropout=0.5):\n",
        "    \"\"\"Create AlexNet with spatial attention\"\"\"\n",
        "    return CNN2D(\n",
        "        in_channels=3,\n",
        "        base_filters=16,\n",
        "        num_classes=num_classes,\n",
        "        dropout=dropout,\n",
        "        attention_type='spatial'\n",
        "    )\n",
        "\n",
        "\n",
        "def create_alexnet_cbam(num_classes=5, dropout=0.5):\n",
        "    \"\"\"Create AlexNet with CBAM (Channel + Spatial attention)\"\"\"\n",
        "    return CNN2D(\n",
        "        in_channels=3,\n",
        "        base_filters=16,\n",
        "        num_classes=num_classes,\n",
        "        dropout=dropout,\n",
        "        attention_type='cbam'\n",
        "    )\n",
        "\n",
        "\n",
        "def create_alexnet_self_attention(num_classes=5, dropout=0.5):\n",
        "    \"\"\"Create AlexNet with self-attention (Conv-based)\"\"\"\n",
        "    return CNN2D(\n",
        "        in_channels=3,\n",
        "        base_filters=16,\n",
        "        num_classes=num_classes,\n",
        "        dropout=dropout,\n",
        "        attention_type='self',\n",
        "        reduction_ratio=8\n",
        "    )\n",
        "\n",
        "print(\"✓ CNN2D (AlexNet) loaded\")\n",
        "print(\"  Available: create_alexnet, create_alexnet_channel_attention,\")\n",
        "print(\"             create_alexnet_spatial_attention, create_alexnet_cbam, create_alexnet_self_attention\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-5E2JON8iRy"
      },
      "source": [
        "## 12. Models - CNN2D ResNet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BdYc9Rtp8iRy"
      },
      "source": [
        "**From: `cnn2d_resnet.py`** - Your custom ResNet implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SxXiLlVs8iRy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Basic Residual Block with skip connection\n",
        "    Used in ResNet-18/34\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, stride=1, downsample=None):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "\n",
        "        # Main path\n",
        "        self.conv1 = nn.Conv2d(in_channels, out_channels,\n",
        "                               kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(out_channels, out_channels,\n",
        "                               kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "        # Skip connection (identity or projection)\n",
        "        self.downsample = downsample\n",
        "\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        self.device =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        self.device = device\n",
        "\n",
        "        identity = x  # Save input for skip connection\n",
        "\n",
        "        # Main path\n",
        "        out = self.conv1(x)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        out = self.conv2(out)\n",
        "        out = self.bn2(out)\n",
        "\n",
        "        # Skip connection\n",
        "        if self.downsample is not None:\n",
        "            identity = self.downsample(x)  # Project to match dimensions\n",
        "\n",
        "        out += identity  # Element-wise addition (the key innovation!)\n",
        "        out = self.relu(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class ResNet18(nn.Module):\n",
        "    \"\"\"\n",
        "    ResNet-18 for ECG image classification\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, num_classes=5, base_filters=16):\n",
        "        super(ResNet18, self).__init__()\n",
        "\n",
        "        # Initial convolution\n",
        "        self.conv1 = nn.Conv2d(in_channels, base_filters, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(base_filters)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
        "\n",
        "        # Residual blocks\n",
        "        self.layer1 = self._make_layer(base_filters, base_filters, 2, stride=1)\n",
        "        self.layer2 = self._make_layer(base_filters, base_filters*2, 2, stride=2)\n",
        "        self.layer3 = self._make_layer(base_filters*2, base_filters*4, 2, stride=2)\n",
        "        self.layer4 = self._make_layer(base_filters*4, base_filters*8, 2, stride=2)\n",
        "\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        self.fc = nn.Linear(base_filters*8, num_classes)\n",
        "\n",
        "    def _make_layer(self, in_channels, out_channels, num_blocks, stride):\n",
        "        \"\"\"Create a composite layer with (num_blocks) residual blocks\"\"\"\n",
        "        downsample = None\n",
        "\n",
        "        # If dimensions change, we need projection for skip connection\n",
        "        if stride != 1 or in_channels != out_channels:\n",
        "            downsample = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "            )\n",
        "\n",
        "        layers = []\n",
        "        # First block may have stride > 1 (downsampling)\n",
        "        layers.append(ResidualBlock(in_channels, out_channels, stride, downsample))\n",
        "\n",
        "        # Remaining blocks have stride=1\n",
        "        for _ in range(1, num_blocks):\n",
        "            layers.append(ResidualBlock(out_channels, out_channels, stride=1, downsample=None))\n",
        "\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, 3, 224, 224)\n",
        "        Returns:\n",
        "            logits: (batch, num_classes)\n",
        "        \"\"\"\n",
        "        # Initial conv\n",
        "        out = self.conv1(x)      # (batch, 16, 112, 112)\n",
        "        out = self.bn1(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.maxpool(out)    # (batch, 16, 56, 56)\n",
        "\n",
        "        # Residual blocks\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "\n",
        "        # Global pooling and FC\n",
        "        out = self.avgpool(out)    # (batch, 128, 1, 1)\n",
        "        out = torch.flatten(out, 1)  # (batch, 128)\n",
        "        logits = self.fc(out)         # (batch, 5)\n",
        "\n",
        "        return logits\n",
        "\n",
        "print(\"✓ ResNet model loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b14dV2w8iRy"
      },
      "source": [
        "## 13. Models - CNN2D VGGNet\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiUB6q_i8iR1"
      },
      "source": [
        "**From: `cnn2d_vggnet.py`** - Your custom VGG implementation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NisHHsKm8iR1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class VGGBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    VGG-style block: multiple Conv layers + ReLU + pooling\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, num_convs):\n",
        "        super(VGGBlock, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        for i in range(num_convs):\n",
        "            layers.append(\n",
        "                nn.Conv2d(in_channels if i==0 else out_channels,\n",
        "                          out_channels,\n",
        "                          kernel_size=3,\n",
        "                          padding=1,\n",
        "                        ))\n",
        "            layers.append(nn.BatchNorm2d(out_channels))\n",
        "            layers.append(nn.ReLU(inplace=True))\n",
        "\n",
        "        # Add MaxPool2d at the end\n",
        "        layers.append(nn.MaxPool2d(kernel_size=2, stride=2))\n",
        "        self.features_block = nn.Sequential(*layers)\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def forward(self, x):\n",
        "        device = x.device\n",
        "        self.device = device\n",
        "\n",
        "        return self.features_block(x)\n",
        "\n",
        "class VGGNet(nn.Module):\n",
        "    \"\"\"\n",
        "    VGG-style network for ECG images\n",
        "    Uses VGG blocks with batch normalization\n",
        "    \"\"\"\n",
        "    def __init__(self, in_channels, out_channels, num_classes=5, dropout=0.5):\n",
        "        super(VGGNet, self).__init__()\n",
        "\n",
        "        self.block1 = VGGBlock(in_channels, in_channels*6, 2)\n",
        "        self.block2 = VGGBlock(in_channels*6, in_channels*12, 2)\n",
        "        self.block3 = VGGBlock(in_channels*12, in_channels*24, 3)\n",
        "        self.block4 = VGGBlock(in_channels*24, in_channels*48, 3)\n",
        "        self.block5 = VGGBlock(in_channels*48, out_channels, 3)\n",
        "\n",
        "        self.adaptmaxpool1 = nn.AdaptiveMaxPool2d((3, 3))\n",
        "\n",
        "        # Classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(out_channels * 3 * 3, 512),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(256, num_classes),\n",
        "        )\n",
        "        self.device =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, 3, 224, 224)\n",
        "        Returns:\n",
        "            logits: (batch, num_classes)\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "        self.device = device\n",
        "\n",
        "        out = self.block1(x)\n",
        "        out = self.block2(out)\n",
        "        out = self.block3(out)\n",
        "        out = self.block4(out)\n",
        "        out = self.block5(out)\n",
        "\n",
        "        # TODO: Adaptive pooling and flatten\n",
        "        out = self.adaptmaxpool1(out)\n",
        "        out = torch.flatten(out, 1)\n",
        "\n",
        "        logits = self.classifier(out)\n",
        "        return logits\n",
        "\n",
        "print(\"✓ VGGNet model loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlOJAfVk8iR1"
      },
      "source": [
        "## 14. Models - CNN1D + RNN Hybrid\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJjquqZs8iR1"
      },
      "source": [
        "**From: `cnn1d_rnn.py`** - Your 1D CNN + RNN hybrid model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NM1R52QG8iR1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from core.models.rnn_block import RNN_BLOCK\n",
        "\n",
        "class CRNN_1D(nn.Module):\n",
        "    \"\"\"\n",
        "    1D Convolutional Recurrent Neural Network.\n",
        "    As described in the official paper.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        input_channels=3,\n",
        "        num_classes=5,\n",
        "        rnn_type='gru',\n",
        "        hidden_size=128,\n",
        "        num_rnn_layers=2,\n",
        "        bidirectional = False,\n",
        "        dropout=0.5\n",
        "    ):\n",
        "        super(CRNN_1D, self).__init__()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.hidden_size = hidden_size\n",
        "        self.bidirectional = bidirectional\n",
        "        self.device =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "        self.cnn_features = nn.Sequential(\n",
        "            nn.Conv1d(input_channels, input_channels*11, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(input_channels*11),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv1d(input_channels*11, input_channels*22, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(input_channels*22),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "\n",
        "            nn.Conv1d(input_channels*22, input_channels*44, kernel_size=5, padding=2),\n",
        "            nn.BatchNorm1d(input_channels*44),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool1d(kernel_size=2, stride=2),\n",
        "        )\n",
        "\n",
        "        # Calculating sequence length after CNN\n",
        "        # Input: 1000; After 3 maxpools (stride=2): 1000/8 = 125\n",
        "        self.seq_len = 125\n",
        "        self.cnn_output_channels = input_channels*44 # 132\n",
        "\n",
        "\n",
        "        self.rnn = RNN_BLOCK(\n",
        "            input_size=self.cnn_output_channels,\n",
        "            hidden_size=hidden_size,\n",
        "            num_layers=num_rnn_layers,\n",
        "            dropout=dropout,\n",
        "            bidirectional=self.bidirectional,\n",
        "            rnn_type=self.rnn_type\n",
        "        )\n",
        "\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(hidden_size * 2, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (batch, 3, 1000) - Raw ECG signals\n",
        "        Returns:\n",
        "            logits: (batch, 5)\n",
        "        \"\"\"\n",
        "        device = x.device\n",
        "        self.device = device\n",
        "\n",
        "        # CNN feature extraction\n",
        "        features = self.cnn_features(x)  # (batch, 132, 125)\n",
        "        out, hidden = self.rnn(features)\n",
        "        # Classification\n",
        "        logits = self.classifier(hidden)  # (batch, 5)\n",
        "\n",
        "        return logits\n",
        "\n",
        "\n",
        "print(\"✓ CNN1D_RNN hybrid model loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S1Nxn4Qk8iR1"
      },
      "source": [
        "## 15. Models - Joint Fusion\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0P9g3n1r8iR1"
      },
      "source": [
        "**From: `joint_fusion.py`** - Your multi-modal fusion model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dADcDviF8iR1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class JointFusion(nn.Module):\n",
        "    \"\"\"\n",
        "    Joint fusion: Combine features from multiple encoders\n",
        "    \"\"\"\n",
        "    def __init__(self,in_channels=3, out_channels=128, encoder_1d=None, encoder_2d=None, num_classes=5):\n",
        "        super(JointFusion, self).__init__()\n",
        "        self.device =  'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "        # Encoder for 1D signals\n",
        "        if encoder_1d:\n",
        "            self.encoder_1d = encoder_1d\n",
        "        else:\n",
        "            self.encoder_1d = nn.Sequential(\n",
        "                nn.Conv1d(in_channels, in_channels*11, kernel_size=5, padding=2),\n",
        "                nn.BatchNorm1d(in_channels*11),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool1d(2),\n",
        "                nn.Conv1d(in_channels*11, in_channels*22, kernel_size=5, padding=2),\n",
        "                nn.BatchNorm1d(in_channels*22),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool1d(2),\n",
        "                nn.Conv1d(in_channels*22, out_channels, kernel_size=5, padding=2),\n",
        "                nn.BatchNorm1d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.AdaptiveAvgPool1d(1)\n",
        "            )\n",
        "\n",
        "        # Encoder for GAF images\n",
        "        if encoder_2d:\n",
        "            self.encoder_2d = encoder_2d\n",
        "        else:\n",
        "            self.encoder_2d = nn.Sequential(\n",
        "                nn.Conv2d(in_channels, in_channels*11, kernel_size=5, padding=2),\n",
        "                nn.BatchNorm2d(in_channels*11),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Conv2d(in_channels*11, in_channels*22, kernel_size=5, padding=2),\n",
        "                nn.BatchNorm2d(in_channels*22),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.MaxPool2d(2),\n",
        "                nn.Conv2d(in_channels*22, out_channels, kernel_size=5, padding=2),\n",
        "                nn.BatchNorm2d(out_channels),\n",
        "                nn.ReLU(inplace=True),\n",
        "                nn.AdaptiveAvgPool2d((1, 1))\n",
        "            )\n",
        "\n",
        "        # Fusion layer\n",
        "        self.fusion = nn.Sequential(\n",
        "            nn.Linear(out_channels*2, 256),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5)\n",
        "        )\n",
        "\n",
        "        # Shared classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(256, 128),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "            nn.Linear(128, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: (signal, image)\n",
        "            signal: (batch, 3, 1000)\n",
        "            image: (batch, 3, 224, 224)\n",
        "        \"\"\"\n",
        "        # device = x.device\n",
        "        # self.device = device\n",
        "\n",
        "        signal, image = x\n",
        "        # Extract features from each modality\n",
        "        feat_1d = self.encoder_1d(signal)  # (batch, 128, 1)\n",
        "        feat_1d = feat_1d.squeeze(-1)          # (batch, 128)\n",
        "\n",
        "        feat_2d = self.encoder_2d(image)  # (batch, 128, 1, 1)\n",
        "        feat_2d = feat_2d.view(feat_2d.size(0), -1)  # (batch, 128)\n",
        "\n",
        "        # Concatenate features\n",
        "        fused = torch.cat([feat_1d, feat_2d], dim=1)  # (batch, 256)\n",
        "\n",
        "        # Fusion layer\n",
        "        fused = self.fusion(fused)  # (batch, 256)\n",
        "\n",
        "        # Classification\n",
        "        logits = self.classifier(fused)\n",
        "        return logits\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"✓ Joint Fusion model loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAdQHnM38iR1"
      },
      "source": [
        "## 16. Training - History Tracker\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9DhIQEhV8iR1"
      },
      "source": [
        "**From: `history.py`** - Your training history tracker\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MkyxQG7D8iR2"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "from datetime import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def save_histories(histories, model_names, save_dir = config.TRAIN_HISTORY_SAVE_PATH):\n",
        "    \"\"\"\n",
        "    Save training histories for each model to disk.\n",
        "\n",
        "    Args:\n",
        "        histories: List of history dictionaries (one per model)\n",
        "        model_names: List of model name strings\n",
        "        save_dir: Directory to save history files (default: core/training/saved)\n",
        "\n",
        "    Returns:\n",
        "        List of saved file paths\n",
        "    \"\"\"\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    saved_files = []\n",
        "\n",
        "    for model_name, history in zip(model_names, histories):\n",
        "        # Create filename with model name and timestamp\n",
        "        filename = f\"{model_name}_history_{timestamp}.pkl\"\n",
        "        filepath = os.path.join(save_dir, filename)\n",
        "\n",
        "        # Save history dict with pickle\n",
        "        with open(filepath, 'wb') as f:\n",
        "            pickle.dump(history, f)\n",
        "\n",
        "        saved_files.append(filepath)\n",
        "        print(f\"âœ“ Saved {model_name} history to {filepath}\")\n",
        "\n",
        "    return saved_files\n",
        "\n",
        "def load_history(filepath):\n",
        "    \"\"\"\n",
        "    Load a single training history from disk.\n",
        "\n",
        "    Args:\n",
        "        filepath: Path to the pickled history file\n",
        "\n",
        "    Returns:\n",
        "        history dictionary\n",
        "    \"\"\"\n",
        "    with open(filepath, 'rb') as f:\n",
        "        history = pickle.load(f)\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def load_all_histories(save_dir = f'{config.TRAIN_HISTORY_SAVE_PATH}', pattern=None):\n",
        "    \"\"\"\n",
        "    Load all or filtered training histories from save directory.\n",
        "\n",
        "    Args:\n",
        "        save_dir: Directory containing history files\n",
        "        pattern: Optional substring to filter files (e.g., 'JointFusion')\n",
        "\n",
        "    Returns:\n",
        "        Dictionary mapping filenames to histories\n",
        "    \"\"\"\n",
        "    histories_dict = {}\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "        print(f\"Directory {save_dir} not found.\")\n",
        "        return histories_dict\n",
        "\n",
        "    for filename in os.listdir(save_dir):\n",
        "        if filename.endswith('.pkl'):\n",
        "            if pattern is None or pattern in filename:\n",
        "                filepath = os.path.join(save_dir, filename)\n",
        "                try:\n",
        "                    history = load_history(filepath)\n",
        "                    histories_dict[filename] = history\n",
        "                    print(f\"âœ“ Loaded {filename}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"âœ— Failed to load {filename}: {e}\")\n",
        "\n",
        "    return histories_dict\n",
        "\n",
        "\n",
        "def plot_training_history(histories, model_names,  save_path = f'{config.TRAIN_HISTORY_SAVE_PATH}/training_history.png'):\n",
        "    \"\"\"\n",
        "    Plot training curves for all models\n",
        "\n",
        "    Args:\n",
        "        histories: List of history dictionaries\n",
        "        model_names: List of model names\n",
        "    \"\"\"\n",
        "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Helper to plot if metric exists\n",
        "    def _plot_metric(ax, key_names, title, ylabel=None):\n",
        "        plotted = False\n",
        "        for history, name in zip(histories, model_names):\n",
        "            for key in key_names:\n",
        "                if key in history:\n",
        "                    ax.plot(history[key], label=name)\n",
        "                    plotted = True\n",
        "                    break\n",
        "        ax.set_title(title)\n",
        "        ax.set_xlabel('Epoch')\n",
        "        if ylabel:\n",
        "            ax.set_ylabel(ylabel)\n",
        "        ax.grid(True)\n",
        "        if plotted:\n",
        "            ax.legend()\n",
        "        else:\n",
        "            ax.text(0.5, 0.5, 'Metric not available', ha='center', va='center', fontsize=10, color='gray')\n",
        "\n",
        "    # Plotting metrics with fallbacks for common key names\n",
        "    _plot_metric(axes[0], ['train_loss', 'loss'], 'Training Loss', ylabel='Loss')\n",
        "    _plot_metric(axes[1], ['val_loss', 'validation_loss'], 'Validation Loss', ylabel='Loss')\n",
        "    _plot_metric(axes[2], ['sensitivity', 'recall', 'sens'], 'Sensitivity / Recall', ylabel='Sensitivity')\n",
        "    _plot_metric(axes[3], ['specificity', 'precision', 'spec'], 'Specificity / Precision', ylabel='Score')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(save_path, dpi=300)\n",
        "    plt.close(fig)\n",
        "    print(f\"âœ“ Saved training curves to {save_path}\")\n",
        "\n",
        "print(\"✓ TrainingHistory loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BmqcEJu38iR2"
      },
      "source": [
        "## 17. Training - Main Training Loop\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNkZueqT8iR2"
      },
      "source": [
        "**From: `training.py`** - Your complete training pipeline\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DEhrW0tf8iR2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Training script for RNNs, CNN2D, CNN1D, and Join Fusion model on ECG Signals + Images\n",
        "Tests if models are learning properly\n",
        "\"\"\"\n",
        "\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import numpy as np\n",
        "# from tqdm import tqdm\n",
        "# import os\n",
        "\n",
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"\n",
        "    Train model for one epoch\n",
        "\n",
        "    Args:\n",
        "        model: model to train/validate\n",
        "        dataloader: Training/Validation data loader\n",
        "        criterion: Loss function\n",
        "        optimizer: Optimizer\n",
        "        device: 'cuda' or 'cpu'\n",
        "\n",
        "    Returns:\n",
        "        avg_loss: Average training loss\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for x, labels in tqdm(dataloader, desc=\"Training\"):\n",
        "        if isinstance(x, (tuple,list)):\n",
        "            x = (x[0].to(device), x[1].to(device)) # (images, images/signals): (batch, 3, 224, 224), (batch, 1000, 3)\n",
        "        else:\n",
        "            x = x.to(device) # (images/signals)\n",
        "        labels = labels.to(device) # (batch, 5)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(x)\n",
        "        if isinstance(output, (tuple, list)):\n",
        "            logits, _ = output\n",
        "        else:\n",
        "            logits = output\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def validate_one_epoch(model, val_loader, criterion, metrics, device):\n",
        "    \"\"\"\n",
        "    Validation phase - calculate LOSS + ALL METRICS\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    # Store ALL predictions and labels for this epoch\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, labels in tqdm(val_loader, desc='Validation'):\n",
        "            if isinstance(x, (tuple,list)):\n",
        "                x = (x[0].to(device), x[1].to(device))\n",
        "            else:\n",
        "                x = x.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            logits = model(x)\n",
        "            output = model(x)\n",
        "            if isinstance(output, (tuple, list)):\n",
        "                logits, _ = output\n",
        "            else:\n",
        "                logits = output\n",
        "            loss = criterion(logits, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Get probabilities and predictions\n",
        "            probs = torch.sigmoid(logits)\n",
        "            preds = (probs > 0.5).float()\n",
        "\n",
        "            # Store for metric calculation\n",
        "            all_predictions.append(preds.cpu().numpy())\n",
        "            all_labels.append(labels.cpu().numpy())\n",
        "            all_probs.append(probs.cpu().numpy())\n",
        "\n",
        "\n",
        "    # Calculate average loss\n",
        "    avg_loss = total_loss / len(val_loader)\n",
        "\n",
        "    # Concatenate all batches\n",
        "    all_predictions = np.vstack(all_predictions)  # (N, 5)\n",
        "    all_labels = np.vstack(all_labels)            # (N, 5)\n",
        "    all_probs = np.vstack(all_probs)              # (N, 5)\n",
        "\n",
        "\n",
        "    # Calculate ALL metrics using predictions from entire epoch\n",
        "    val_metrics = metrics.calculate_metrics(\n",
        "        all_labels,\n",
        "        all_predictions,\n",
        "        all_probs\n",
        "    )\n",
        "\n",
        "    return avg_loss, val_metrics\n",
        "\n",
        "\n",
        "def train_model(model,\n",
        "                model_name,\n",
        "                train_loader,\n",
        "                val_loader,\n",
        "                criterion,\n",
        "                optimizer,\n",
        "                schedular,\n",
        "                metrics,\n",
        "                config\n",
        "                ):\n",
        "    \"\"\"\n",
        "    Complete training loop\n",
        "    \"\"\"\n",
        "    history = {\n",
        "        'train_loss': [],\n",
        "        'val_loss': [],\n",
        "        'sensitivity': [],\n",
        "        'specificity': [],\n",
        "        'precision': [],\n",
        "        'f1': [],\n",
        "        'accuracy': [],\n",
        "        'auc_roc': [],\n",
        "        'auc_pr': [],\n",
        "    }\n",
        "    class_wise_metrics = {cls: {'sensitivity': [], 'specificity': [], 'precision': [], 'f1': [], 'accuracy': [], 'auc_roc': [], 'auc_pr': []} for cls in config.CLASS_NAMES}\n",
        "    best_val_loss = float('inf')\n",
        "    for epoch in range(config.NUM_EPOCHS):\n",
        "        print(f\"\\nEpoch {epoch+1}/{config.NUM_EPOCHS}\")\n",
        "\n",
        "        # Training - get only loss\n",
        "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, config.DEVICE)\n",
        "\n",
        "        # Validation - get loss + all metrics\n",
        "        val_loss, val_metrics = validate_one_epoch(model, val_loader, criterion, metrics, config.DEVICE)\n",
        "\n",
        "        schedular.step(val_loss)\n",
        "\n",
        "        # Store history\n",
        "        history['train_loss'].append(train_loss)\n",
        "        history['val_loss'].append(val_loss)\n",
        "        history['sensitivity'].append(val_metrics['macro_avg']['sensitivity'])\n",
        "        history['specificity'].append(val_metrics['macro_avg']['specificity'])\n",
        "        history['precision'].append(val_metrics['macro_avg']['precision'])\n",
        "        history['f1'].append(val_metrics['macro_avg']['f1_score'])\n",
        "        history['accuracy'].append(val_metrics['macro_avg']['accuracy'])\n",
        "        history['auc_roc'].append(val_metrics['macro_avg']['auc_roc'])\n",
        "        history['auc_pr'].append(val_metrics['macro_avg']['auc_pr'])\n",
        "\n",
        "        for cls in config.CLASS_NAMES:\n",
        "            class_wise_metrics[cls]['sensitivity'].append(val_metrics[cls]['sensitivity'])\n",
        "            class_wise_metrics[cls]['specificity'].append(val_metrics[cls]['specificity'])\n",
        "            class_wise_metrics[cls]['precision'].append(val_metrics[cls]['precision'])\n",
        "            class_wise_metrics[cls]['f1'].append(val_metrics[cls]['f1_score'])\n",
        "            class_wise_metrics[cls]['accuracy'].append(val_metrics[cls]['accuracy'])\n",
        "            class_wise_metrics[cls]['auc_roc'].append(val_metrics[cls]['auc_roc'])\n",
        "            class_wise_metrics[cls]['auc_pr'].append(val_metrics[cls]['auc_pr'])\n",
        "\n",
        "        # Print summary\n",
        "        print(f\"Train Loss: {train_loss:.4f}\")\n",
        "        print(f\"Val Loss: {val_loss:.4f}\")\n",
        "        print(f\"Val Metrics (Macro):\")\n",
        "        print(f\"  Sensitivity: {val_metrics['macro_avg']['sensitivity']:.4f}\")\n",
        "        print(f\"  Specificity: {val_metrics['macro_avg']['specificity']:.4f}\")\n",
        "        print(f\"  Precision: {val_metrics['macro_avg']['precision']:.4f}\")\n",
        "        print(f\"  F1-Score: {val_metrics['macro_avg']['f1_score']:.4f}\")\n",
        "        print(f\"  Accuracy: {val_metrics['macro_avg']['accuracy']:.4f}\")\n",
        "        print(f\"  AUC-ROC: {val_metrics['macro_avg']['auc_roc']:.4f}\")\n",
        "        print(f\"  AUC-PR: {val_metrics['macro_avg']['auc_pr']:.4f}\")\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            best_epoch = epoch\n",
        "            patience_counter = 0\n",
        "\n",
        "            if config.SAVE_BEST == True:\n",
        "                save_path = os.path.join(config.SAVE_DIR, f'{model_name}_best.pth')\n",
        "                torch.save({\n",
        "                    'epoch': best_epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'val_loss': val_loss,\n",
        "                    'val_metrics': val_metrics,\n",
        "                    'history': history\n",
        "                }, save_path)\n",
        "\n",
        "                print(f\"âœ“ Saved best model (Val Loss: {val_loss:.4f})\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        # Early stopping\n",
        "        if patience_counter >= config.PATIENCE:\n",
        "            print(f\"\\nEarly stopping triggered after {epoch+1} epochs\")\n",
        "            print(f\"Best epoch: {best_epoch+1} with Val Loss: {best_val_loss:.4f}\")\n",
        "            break\n",
        "\n",
        "        print(f\"\\n{'='*45}\")\n",
        "        print(f\"Training complete!\")\n",
        "        print(f\"Best epoch: {best_epoch+1}\")\n",
        "        print(f\"Best val loss: {best_val_loss:.4f}\")\n",
        "        print(f\"{'='*45}\")\n",
        "\n",
        "    return history, class_wise_metrics\n",
        "\n",
        "print(\"✓ Training functions loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zee0ZDTi8iR2"
      },
      "source": [
        "## 18. Metrics & Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A3yjDuoO8iR2"
      },
      "source": [
        "**From: `metrics.py`** - Your comprehensive metrics system\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IbxwO5gK8iR2"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Metrics Module for ECG Classification.\n",
        "Includes confusion matrix, ROC curves, and comprehensive model comparison.\n",
        "\"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    average_precision_score,\n",
        "    precision_recall_curve,\n",
        "    accuracy_score\n",
        ")\n",
        "import pandas as pd\n",
        "from typing import Dict, List, Tuple\n",
        "import os\n",
        "\n",
        "\n",
        "class ECGMetrics:\n",
        "    \"\"\"Comprehensive metrics calculator for ECG classification\"\"\"\n",
        "\n",
        "    def __init__(self, class_names=['NORM', 'MI', 'STTC', 'CD', 'HYP']):\n",
        "        self.class_names = class_names\n",
        "        self.num_classes = len(class_names)\n",
        "\n",
        "    def calculate_confusion_matrix(self, y_true, y_pred):\n",
        "        \"\"\"\n",
        "        Calculate confusion matrix for each class (one-vs-rest)\n",
        "\n",
        "        Args:\n",
        "            y_true: (N, num_classes) binary labels\n",
        "            y_pred: (N, num_classes) binary predictions\n",
        "\n",
        "        Returns:\n",
        "            dict: Confusion matrices for each class\n",
        "        \"\"\"\n",
        "        cm_dict = {}\n",
        "\n",
        "        for i, class_name in enumerate(self.class_names):\n",
        "            cm = confusion_matrix(y_true[:, i], y_pred[:, i])\n",
        "            cm_dict[class_name] = cm\n",
        "\n",
        "        return cm_dict\n",
        "\n",
        "    def calculate_metrics(self, y_true, y_pred, y_prob):\n",
        "        \"\"\"\n",
        "        Calculate comprehensive metrics for multi-label classification\n",
        "\n",
        "        Args:\n",
        "            y_true: (N, num_classes) true binary labels\n",
        "            y_pred: (N, num_classes) predicted binary labels\n",
        "            y_prob: (N, num_classes) predicted probabilities\n",
        "\n",
        "        Returns:\n",
        "            dict: All metrics organized by class and averages\n",
        "        \"\"\"\n",
        "        metrics = {}\n",
        "\n",
        "        # Per-class metrics\n",
        "        for i, class_name in enumerate(self.class_names):\n",
        "            yt = y_true[:, i]\n",
        "            yp = y_pred[:, i]\n",
        "            yprob = y_prob[:, i]\n",
        "\n",
        "            # Confusion matrix elements\n",
        "            cm = confusion_matrix(yt, yp)\n",
        "            if cm.shape == (2, 2):\n",
        "                tn, fp, fn, tp = cm.ravel()\n",
        "            else:\n",
        "                # Handle edge case where only one class is present\n",
        "                tp = fp = tn = fn = 0\n",
        "                if cm.shape == (1, 1):\n",
        "                    if yt[0] == 1:\n",
        "                        tp = cm[0, 0]\n",
        "                    else:\n",
        "                        tn = cm[0, 0]\n",
        "\n",
        "            # Calculate metrics\n",
        "            sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
        "            specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
        "            precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
        "            npv = tn / (tn + fn) if (tn + fn) > 0 else 0.0  # Negative Predictive Value\n",
        "\n",
        "            f1 = f1_score(yt, yp, zero_division=0)\n",
        "            accuracy = accuracy_score(yt, yp)\n",
        "\n",
        "            # AUC metrics\n",
        "            try:\n",
        "                auc_roc = roc_auc_score(yt, yprob)\n",
        "                auc_pr = average_precision_score(yt, yprob)\n",
        "            except:\n",
        "                auc_roc = 0.0\n",
        "                auc_pr = 0.0\n",
        "\n",
        "            metrics[class_name] = {\n",
        "                'confusion_matrix': cm,\n",
        "                'tp': int(tp),\n",
        "                'tn': int(tn),\n",
        "                'fp': int(fp),\n",
        "                'fn': int(fn),\n",
        "                'sensitivity': float(sensitivity),\n",
        "                'specificity': float(specificity),\n",
        "                'precision': float(precision),\n",
        "                'npv': float(npv),\n",
        "                'f1_score': float(f1),\n",
        "                'accuracy': float(accuracy),\n",
        "                'auc_roc': float(auc_roc),\n",
        "                'auc_pr': float(auc_pr),\n",
        "                'support': int(yt.sum())\n",
        "            }\n",
        "\n",
        "        # Calculate macro averages\n",
        "        metrics['macro_avg'] = {\n",
        "            'sensitivity': np.mean([m['sensitivity'] for m in metrics.values() if isinstance(m, dict)]),\n",
        "            'specificity': np.mean([m['specificity'] for m in metrics.values() if isinstance(m, dict)]),\n",
        "            'precision': np.mean([m['precision'] for m in metrics.values() if isinstance(m, dict)]),\n",
        "            'f1_score': np.mean([m['f1_score'] for m in metrics.values() if isinstance(m, dict)]),\n",
        "            'accuracy': np.mean([m['accuracy'] for m in metrics.values() if isinstance(m, dict)]),\n",
        "            'auc_roc': np.mean([m['auc_roc'] for m in metrics.values() if isinstance(m, dict)]),\n",
        "            'auc_pr': np.mean([m['auc_pr'] for m in metrics.values() if isinstance(m, dict)])\n",
        "        }\n",
        "\n",
        "        # Calculate micro averages (treating all classes equally)\n",
        "        y_true_flat = y_true.ravel()\n",
        "        y_pred_flat = y_pred.ravel()\n",
        "        y_prob_flat = y_prob.ravel()\n",
        "\n",
        "        return metrics\n",
        "\n",
        "\n",
        "    def plot_confusion_matrices(self, y_true, y_pred, save_path = f'{config.METRICS_SAVE_PATH}/confusion_matrices.png',\n",
        "                                model_name='Model'):\n",
        "        \"\"\"\n",
        "        Plot confusion matrices for all classes in a grid\n",
        "\n",
        "        Args:\n",
        "            y_true: (N, num_classes) true labels\n",
        "            y_pred: (N, num_classes) predictions\n",
        "            save_path: Path to save figure\n",
        "            model_name: Name of the model for title\n",
        "        \"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i, class_name in enumerate(self.class_names):\n",
        "            cm = confusion_matrix(y_true[:, i], y_pred[:, i])\n",
        "\n",
        "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                       xticklabels=['Negative', 'Positive'],\n",
        "                       yticklabels=['Negative', 'Positive'],\n",
        "                       ax=axes[i], cbar=True)\n",
        "\n",
        "            axes[i].set_title(f'{class_name}', fontsize=12, fontweight='bold')\n",
        "            axes[i].set_ylabel('True Label')\n",
        "            axes[i].set_xlabel('Predicted Label')\n",
        "\n",
        "        # Hide the last subplot if we have 5 classes\n",
        "        axes[5].axis('off')\n",
        "\n",
        "        plt.suptitle(f'Confusion Matrices - {model_name}',\n",
        "                    fontsize=16, fontweight='bold', y=0.995)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Saved confusion matrices to {save_path}\")\n",
        "\n",
        "    def plot_roc_curves(self, y_true, y_prob, save_path = f'{config.METRICS_SAVE_PATH}/roc_curves.png',\n",
        "                       model_name='Model'):\n",
        "        \"\"\"\n",
        "        Plot ROC curves for all classes\n",
        "\n",
        "        Args:\n",
        "            y_true: (N, num_classes) true labels\n",
        "            y_prob: (N, num_classes) predicted probabilities\n",
        "            save_path: Path to save figure\n",
        "            model_name: Name of the model\n",
        "        \"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i, class_name in enumerate(self.class_names):\n",
        "            try:\n",
        "                fpr, tpr, _ = roc_curve(y_true[:, i], y_prob[:, i])\n",
        "                auc = roc_auc_score(y_true[:, i], y_prob[:, i])\n",
        "\n",
        "                axes[i].plot(fpr, tpr, label=f'AUC = {auc:.3f}', linewidth=2)\n",
        "                axes[i].plot([0, 1], [0, 1], 'k--', label='Random', linewidth=1)\n",
        "                axes[i].set_xlabel('False Positive Rate')\n",
        "                axes[i].set_ylabel('True Positive Rate')\n",
        "                axes[i].set_title(f'{class_name}', fontweight='bold')\n",
        "                axes[i].legend(loc='lower right')\n",
        "                axes[i].grid(True, alpha=0.3)\n",
        "            except:\n",
        "                axes[i].text(0.5, 0.5, 'Insufficient data',\n",
        "                           ha='center', va='center')\n",
        "                axes[i].set_title(f'{class_name}', fontweight='bold')\n",
        "\n",
        "        axes[5].axis('off')\n",
        "\n",
        "        plt.suptitle(f'ROC Curves - {model_name}',\n",
        "                    fontsize=16, fontweight='bold', y=0.995)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Saved ROC curves to {save_path}\")\n",
        "\n",
        "    def plot_precision_recall_curves(self, y_true, y_prob,\n",
        "                                     save_path = f'{config.METRICS_SAVE_PATH}/pr_curves.png',\n",
        "                                     model_name='Model'):\n",
        "        \"\"\"\n",
        "        Plot Precision-Recall curves for all classes\n",
        "\n",
        "        Args:\n",
        "            y_true: (N, num_classes) true labels\n",
        "            y_prob: (N, num_classes) predicted probabilities\n",
        "            save_path: Path to save figure\n",
        "            model_name: Name of the model\n",
        "        \"\"\"\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for i, class_name in enumerate(self.class_names):\n",
        "            try:\n",
        "                precision, recall, _ = precision_recall_curve(y_true[:, i], y_prob[:, i])\n",
        "                auc_pr = average_precision_score(y_true[:, i], y_prob[:, i])\n",
        "\n",
        "                axes[i].plot(recall, precision,\n",
        "                           label=f'AP = {auc_pr:.3f}', linewidth=2)\n",
        "                axes[i].set_xlabel('Recall')\n",
        "                axes[i].set_ylabel('Precision')\n",
        "                axes[i].set_title(f'{class_name}', fontweight='bold')\n",
        "                axes[i].legend(loc='lower left')\n",
        "                axes[i].grid(True, alpha=0.3)\n",
        "                axes[i].set_xlim([0, 1])\n",
        "                axes[i].set_ylim([0, 1])\n",
        "            except:\n",
        "                axes[i].text(0.5, 0.5, 'Insufficient data',\n",
        "                           ha='center', va='center')\n",
        "                axes[i].set_title(f'{class_name}', fontweight='bold')\n",
        "\n",
        "        axes[5].axis('off')\n",
        "\n",
        "        plt.suptitle(f'Precision-Recall Curves - {model_name}',\n",
        "                    fontsize=16, fontweight='bold', y=0.995)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Saved PR curves to {save_path}\")\n",
        "\n",
        "    def generate_metrics_report(self, metrics, save_path = f'{config.METRICS_SAVE_PATH}/metrics_report.txt'):\n",
        "        \"\"\"\n",
        "        Generate a detailed text report of all metrics, for a single epoch.\n",
        "\n",
        "        Args:\n",
        "            metrics: Dictionary from calculate_all_metrics()\n",
        "            save_path: Path to save report\n",
        "        \"\"\"\n",
        "        with open(save_path, 'w') as f:\n",
        "            f.write(\"=\" * 80 + \"\\n\")\n",
        "            f.write(\"ECG CLASSIFICATION METRICS REPORT\\n\")\n",
        "            f.write(\"=\" * 80 + \"\\n\\n\")\n",
        "\n",
        "            # Per-class metrics\n",
        "            f.write(\"PER-CLASS METRICS:\\n\")\n",
        "            f.write(\"-\" * 80 + \"\\n\")\n",
        "\n",
        "            for class_name in self.class_names:\n",
        "                m = metrics[class_name]\n",
        "                f.write(f\"\\n{class_name}:\\n\")\n",
        "                f.write(f\"  Confusion Matrix:\\n\")\n",
        "                f.write(f\"    TN: {m['tn']:6d}  FP: {m['fp']:6d}\\n\")\n",
        "                f.write(f\"    FN: {m['fn']:6d}  TP: {m['tp']:6d}\\n\")\n",
        "                f.write(f\"  Metrics:\\n\")\n",
        "                f.write(f\"    Sensitivity (Recall): {m['sensitivity']:.4f}\\n\")\n",
        "                f.write(f\"    Specificity:          {m['specificity']:.4f}\\n\")\n",
        "                f.write(f\"    Precision:            {m['precision']:.4f}\\n\")\n",
        "                f.write(f\"    NPV:                  {m['npv']:.4f}\\n\")\n",
        "                f.write(f\"    F1-Score:             {m['f1_score']:.4f}\\n\")\n",
        "                f.write(f\"    Accuracy:             {m['accuracy']:.4f}\\n\")\n",
        "                f.write(f\"    AUC-ROC:              {m['auc_roc']:.4f}\\n\")\n",
        "                f.write(f\"    AUC-PR:               {m['auc_pr']:.4f}\\n\")\n",
        "                f.write(f\"    Support:              {m['support']}\\n\")\n",
        "\n",
        "            # Macro averages\n",
        "            f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "            f.write(\"MACRO AVERAGES (Equal weight per class):\\n\")\n",
        "            f.write(\"-\" * 80 + \"\\n\")\n",
        "            m = metrics['macro_avg']\n",
        "            f.write(f\"  Sensitivity: {m['sensitivity']:.4f}\\n\")\n",
        "            f.write(f\"  Specificity: {m['specificity']:.4f}\\n\")\n",
        "            f.write(f\"  Precision:   {m['precision']:.4f}\\n\")\n",
        "            f.write(f\"  F1-Score:    {m['f1_score']:.4f}\\n\")\n",
        "            f.write(f\"  Accuracy:    {m['accuracy']:.4f}\\n\")\n",
        "            f.write(f\"  AUC-ROC:     {m['auc_roc']:.4f}\\n\")\n",
        "            f.write(f\"  AUC-PR:      {m['auc_pr']:.4f}\\n\")\n",
        "\n",
        "            # Micro averages\n",
        "            f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "            f.write(\"MICRO AVERAGES (Equal weight per sample):\\n\")\n",
        "            f.write(\"-\" * 80 + \"\\n\")\n",
        "            m = metrics['micro_avg']\n",
        "            f.write(f\"  Sensitivity: {m['sensitivity']:.4f}\\n\")\n",
        "            f.write(f\"  Precision:   {m['precision']:.4f}\\n\")\n",
        "            f.write(f\"  F1-Score:    {m['f1_score']:.4f}\\n\")\n",
        "            f.write(f\"  AUC-ROC:     {m['auc_roc']:.4f}\\n\")\n",
        "\n",
        "            f.write(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
        "\n",
        "        print(f\"Saved metrics report to {save_path}\")\n",
        "\n",
        "print(\"✓ Metrics system loaded\")\n",
        "print(\"  Available: ECGMetrics\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelComparison:\n",
        "    \"\"\"Compare multiple models' performance\"\"\"\n",
        "\n",
        "    def __init__(self, class_names=['NORM', 'MI', 'STTC', 'CD', 'HYP']):\n",
        "        self.class_names = class_names\n",
        "\n",
        "    def compare_models(self, models_metrics: Dict[str, Dict], save_path = f'{config.METRICS_SAVE_PATH}/model_comparison.png'):\n",
        "        \"\"\"\n",
        "        Create comparison visualizations for multiple models\n",
        "\n",
        "        Args:\n",
        "            models_metrics: Dict mapping model_name -> metrics dict\n",
        "            save_path: Path to save comparison plot\n",
        "        \"\"\"\n",
        "        # Extract data for plotting\n",
        "        model_names = list(models_metrics.keys())\n",
        "        metrics_to_plot = ['sensitivity', 'specificity', 'precision',\n",
        "                          'f1_score', 'auc_roc']\n",
        "\n",
        "        # Create subplots\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        # Plot per-class comparisons\n",
        "        for i, class_name in enumerate(self.class_names):\n",
        "            ax = axes[i]\n",
        "\n",
        "            # Prepare data\n",
        "            data = {metric: [] for metric in metrics_to_plot}\n",
        "\n",
        "            for model_name in model_names:\n",
        "                metrics = models_metrics[model_name][class_name]\n",
        "                for metric in metrics_to_plot:\n",
        "                    data[metric].append(metrics[metric])\n",
        "\n",
        "            # Create grouped bar plot\n",
        "            x = np.arange(len(model_names))\n",
        "            width = 0.15\n",
        "\n",
        "            for j, metric in enumerate(metrics_to_plot):\n",
        "                offset = (j - 2) * width\n",
        "                ax.bar(x + offset, data[metric], width,\n",
        "                      label=metric.replace('_', ' ').title())\n",
        "\n",
        "            ax.set_xlabel('Model')\n",
        "            ax.set_ylabel('Score')\n",
        "            ax.set_title(f'{class_name}', fontweight='bold')\n",
        "            ax.set_xticks(x)\n",
        "            ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
        "            ax.legend(fontsize=8)\n",
        "            ax.grid(True, alpha=0.3, axis='y')\n",
        "            ax.set_ylim([0, 1])\n",
        "\n",
        "        # Plot macro averages comparison\n",
        "        ax = axes[5]\n",
        "        data = {metric: [] for metric in metrics_to_plot}\n",
        "\n",
        "        for model_name in model_names:\n",
        "            metrics = models_metrics[model_name]['macro_avg']\n",
        "            for metric in metrics_to_plot:\n",
        "                data[metric].append(metrics[metric])\n",
        "\n",
        "        x = np.arange(len(model_names))\n",
        "        width = 0.15\n",
        "\n",
        "        for j, metric in enumerate(metrics_to_plot):\n",
        "            offset = (j - 2) * width\n",
        "            ax.bar(x + offset, data[metric], width,\n",
        "                  label=metric.replace('_', ' ').title())\n",
        "\n",
        "        ax.set_xlabel('Model')\n",
        "        ax.set_ylabel('Score')\n",
        "        ax.set_title('Macro Average', fontweight='bold')\n",
        "        ax.set_xticks(x)\n",
        "        ax.set_xticklabels(model_names, rotation=45, ha='right')\n",
        "        ax.legend(fontsize=8)\n",
        "        ax.grid(True, alpha=0.3, axis='y')\n",
        "        ax.set_ylim([0, 1])\n",
        "\n",
        "        plt.suptitle('Model Comparison Across All Classes',\n",
        "                    fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Saved model comparison to {save_path}\")\n",
        "\n",
        "    def create_comparison_table(self, models_metrics: Dict[str, Dict],\n",
        "                                save_path = f'{config.METRICS_SAVE_PATH}/comparison_table.csv'):\n",
        "        \"\"\"\n",
        "        Create a CSV table comparing all models\n",
        "\n",
        "        Args:\n",
        "            models_metrics: Dict mapping model_name -> metrics dict\n",
        "            save_path: Path to save CSV\n",
        "        \"\"\"\n",
        "        rows = []\n",
        "\n",
        "        for model_name, metrics in models_metrics.items():\n",
        "            # Macro averages row\n",
        "            row = {\n",
        "                'Model': model_name,\n",
        "                'Class': 'MACRO AVG',\n",
        "                'Sensitivity': metrics['macro_avg']['sensitivity'],\n",
        "                'Specificity': metrics['macro_avg']['specificity'],\n",
        "                'Precision': metrics['macro_avg']['precision'],\n",
        "                'F1-Score': metrics['macro_avg']['f1_score'],\n",
        "                'AUC-ROC': metrics['macro_avg']['auc_roc'],\n",
        "                'AUC-PR': metrics['macro_avg']['auc_pr']\n",
        "            }\n",
        "            rows.append(row)\n",
        "\n",
        "            # Per-class rows\n",
        "            for class_name in self.class_names:\n",
        "                m = metrics[class_name]\n",
        "                row = {\n",
        "                    'Model': model_name,\n",
        "                    'Class': class_name,\n",
        "                    'Sensitivity': m['sensitivity'],\n",
        "                    'Specificity': m['specificity'],\n",
        "                    'Precision': m['precision'],\n",
        "                    'F1-Score': m['f1_score'],\n",
        "                    'AUC-ROC': m['auc_roc'],\n",
        "                    'AUC-PR': m['auc_pr']\n",
        "                }\n",
        "                rows.append(row)\n",
        "\n",
        "        df = pd.DataFrame(rows)\n",
        "        df.to_csv(save_path, index=False, float_format='%.4f')\n",
        "\n",
        "        print(f\"Saved comparison table to {save_path}\")\n",
        "\n",
        "        return df\n",
        "\n",
        "    def plot_metric_heatmap(self, models_metrics: Dict[str, Dict],\n",
        "                           metric='f1_score', save_path = f'{config.METRICS_SAVE_PATH}/metric_heatmap.png'):\n",
        "        \"\"\"\n",
        "        Create a heatmap showing a specific metric across models and classes\n",
        "\n",
        "        Args:\n",
        "            models_metrics: Dict mapping model_name -> metrics dict\n",
        "            metric: Which metric to visualize\n",
        "            save_path: Path to save figure\n",
        "        \"\"\"\n",
        "        model_names = list(models_metrics.keys())\n",
        "\n",
        "        # Build matrix: rows = models, cols = classes + macro avg\n",
        "        cols = self.class_names + ['Macro Avg']\n",
        "        data = []\n",
        "\n",
        "        for model_name in model_names:\n",
        "            row = []\n",
        "            for class_name in self.class_names:\n",
        "                row.append(models_metrics[model_name][class_name][metric])\n",
        "            row.append(models_metrics[model_name]['macro_avg'][metric])\n",
        "            data.append(row)\n",
        "\n",
        "        data = np.array(data)\n",
        "\n",
        "        # Create heatmap\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        sns.heatmap(data, annot=True, fmt='.3f', cmap='YlGnBu',\n",
        "                   xticklabels=cols, yticklabels=model_names,\n",
        "                   cbar_kws={'label': metric.replace('_', ' ').title()},\n",
        "                   vmin=0, vmax=1)\n",
        "\n",
        "        plt.title(f'{metric.replace(\"_\", \" \").title()} Comparison Across Models',\n",
        "                 fontsize=14, fontweight='bold')\n",
        "        plt.xlabel('Class')\n",
        "        plt.ylabel('Model')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Saved metric heatmap to {save_path}\")\n",
        "    print(\"  Available: ModelComparison\")"
      ],
      "metadata": {
        "id": "6d006Anq-e1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingVisualizer:\n",
        "    \"\"\"Visualize training progress and convergence across epochs.\n",
        "\n",
        "    Works with flattened history structure:\n",
        "    history = {\n",
        "        'train_loss': [...],\n",
        "        'val_loss': [...],\n",
        "        'sensitivity': [...],\n",
        "        'specificity': [...],\n",
        "        'precision': [...],\n",
        "        'f1': [...],\n",
        "        'accuracy': [...],\n",
        "        'auc_roc': [...],\n",
        "        'auc_pr': [...]\n",
        "    }\n",
        "\n",
        "    class_wise_metrics = {\n",
        "        'NORM': {'sensitivity': [...], 'f1': [...], 'auc_roc': [...], ...},\n",
        "        'MI': {...},\n",
        "        ...\n",
        "    }\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, class_names=['NORM', 'MI', 'STTC', 'CD', 'HYP']):\n",
        "        self.class_names = class_names\n",
        "\n",
        "    def plot_loss_curves(self, history, save_path = f'{config.TRAIN_HISTORY_SAVE_PATH}/loss_curves.png', model_name='Model'):\n",
        "        \"\"\"\n",
        "        Plot training and validation loss over epochs\n",
        "\n",
        "        Args:\n",
        "            history: Dict with 'train_loss' and 'val_loss' lists\n",
        "            save_path: Path to save figure\n",
        "            model_name: Name of the model for title\n",
        "        \"\"\"\n",
        "        if 'train_loss' not in history or 'val_loss' not in history:\n",
        "            print(\"Warning: 'train_loss' or 'val_loss' not found in history.\")\n",
        "            return\n",
        "\n",
        "        epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(epochs, history['train_loss'], 'o-', label='Train Loss', linewidth=2, markersize=6)\n",
        "        plt.plot(epochs, history['val_loss'], 's-', label='Val Loss', linewidth=2, markersize=6)\n",
        "        plt.xlabel('Epoch', fontsize=12)\n",
        "        plt.ylabel('Loss', fontsize=12)\n",
        "        plt.title(f'Training and Validation Loss - {model_name}', fontsize=14, fontweight='bold')\n",
        "        plt.legend(fontsize=11)\n",
        "        plt.grid(True, alpha=0.3)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Saved loss curves to {save_path}\")\n",
        "\n",
        "    def plot_macro_metrics(self, history, save_path = f'{config.TRAIN_HISTORY_SAVE_PATH}/macro_metrics.png', model_name='Model'):\n",
        "        \"\"\"\n",
        "        Plot macro-averaged metrics over epochs\n",
        "\n",
        "        Args:\n",
        "            history: Dict with metric keys containing per-epoch lists\n",
        "                    {'sensitivity': [...], 'specificity': [...], 'f1': [...], 'auc_roc': [...], ...}\n",
        "            save_path: Path to save figure\n",
        "            model_name: Name of the model\n",
        "        \"\"\"\n",
        "        # Map history keys to display names (note: 'f1' in history, 'f1_score' for display)\n",
        "        metric_mapping = {\n",
        "            'sensitivity': 'sensitivity',\n",
        "            'specificity': 'specificity',\n",
        "            'precision': 'precision',\n",
        "            'f1': 'f1_score',\n",
        "            'accuracy': 'accuracy',\n",
        "            'auc_roc': 'auc_roc',\n",
        "            'auc_pr': 'auc_pr'\n",
        "        }\n",
        "\n",
        "        # Extract available metrics\n",
        "        metrics_to_plot = {}\n",
        "        for hist_key, display_key in metric_mapping.items():\n",
        "            if hist_key in history and history[hist_key]:\n",
        "                metrics_to_plot[display_key] = history[hist_key]\n",
        "\n",
        "        if not metrics_to_plot:\n",
        "            print(\"Warning: No metrics found in history. Skipping plot_macro_metrics.\")\n",
        "            return\n",
        "\n",
        "        epochs = range(1, len(next(iter(metrics_to_plot.values()))) + 1)\n",
        "\n",
        "        # Create subplots\n",
        "        num_metrics = len(metrics_to_plot)\n",
        "        fig, axes = plt.subplots(2, 4, figsize=(18, 10))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        colors = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f']\n",
        "\n",
        "        for idx, (metric_name, values) in enumerate(metrics_to_plot.items()):\n",
        "            ax = axes[idx]\n",
        "            ax.plot(epochs, values, 'o-',\n",
        "                   color=colors[idx % len(colors)], linewidth=2, markersize=6)\n",
        "            ax.set_xlabel('Epoch', fontsize=10)\n",
        "            ax.set_ylabel('Score', fontsize=10)\n",
        "            ax.set_title(metric_name.replace('_', ' ').title(), fontsize=11, fontweight='bold')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "            ax.set_ylim([0, 1])\n",
        "\n",
        "        # Hide extra subplots\n",
        "        for idx in range(len(metrics_to_plot), len(axes)):\n",
        "            axes[idx].axis('off')\n",
        "\n",
        "        plt.suptitle(f'Macro-Averaged Metrics Over Epochs - {model_name}',\n",
        "                    fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Saved macro metrics curves to {save_path}\")\n",
        "\n",
        "    def plot_per_class_metric(self, class_wise_metrics, metric='f1', class_name=None,\n",
        "                              save_path = f'{config.TRAIN_HISTORY_SAVE_PATH}/per_class_metrics.png', model_name='Model'):\n",
        "        \"\"\"\n",
        "        Plot a specific metric for a specific class over epochs\n",
        "\n",
        "        Args:\n",
        "            class_wise_metrics: Dict with class_name -> {metric: [...], ...}\n",
        "            metric: Metric name to plot (e.g., 'f1', 'sensitivity', 'auc_roc')\n",
        "            class_name: Class to plot. If None, plots all classes in subplots\n",
        "            save_path: Path to save figure. If None, auto-generated from metric and class\n",
        "            model_name: Name of the model\n",
        "        \"\"\"\n",
        "        if not class_wise_metrics:\n",
        "            print(\"Warning: class_wise_metrics is empty. Skipping plot_per_class_metric.\")\n",
        "            return\n",
        "\n",
        "        if class_name is not None:\n",
        "            # Plot single class\n",
        "            if class_name not in class_wise_metrics:\n",
        "                print(f\"Warning: {class_name} not found in class_wise_metrics.\")\n",
        "                return\n",
        "\n",
        "            if metric not in class_wise_metrics[class_name]:\n",
        "                print(f\"Warning: {metric} not found for {class_name}.\")\n",
        "                return\n",
        "\n",
        "            if save_path is None:\n",
        "                save_path = f\"{metric}_{class_name}_over_epochs.png\"\n",
        "\n",
        "            values = class_wise_metrics[class_name][metric]\n",
        "            epochs = range(1, len(values) + 1)\n",
        "\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.plot(epochs, values, 'o-', linewidth=2, markersize=6, color='#1f77b4')\n",
        "            plt.xlabel('Epoch', fontsize=12)\n",
        "            plt.ylabel(metric.replace('_', ' ').title(), fontsize=12)\n",
        "            plt.title(f'{class_name} - {metric.replace(\"_\", \" \").title()} Over Epochs - {model_name}',\n",
        "                     fontsize=13, fontweight='bold')\n",
        "            plt.grid(True, alpha=0.3)\n",
        "            plt.ylim([0, 1])\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            print(f\"Saved {metric} curve for {class_name} to {save_path}\")\n",
        "\n",
        "        else:\n",
        "            # Plot all classes in subplots\n",
        "            if save_path is None:\n",
        "                save_path = f\"{metric}_all_classes_over_epochs.png\"\n",
        "\n",
        "            fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
        "            axes = axes.flatten()\n",
        "\n",
        "            for i, cls_name in enumerate(self.class_names):\n",
        "                ax = axes[i]\n",
        "\n",
        "                if cls_name in class_wise_metrics and metric in class_wise_metrics[cls_name]:\n",
        "                    values = class_wise_metrics[cls_name][metric]\n",
        "                    epochs = range(1, len(values) + 1)\n",
        "                    ax.plot(epochs, values, 'o-', linewidth=2, markersize=6)\n",
        "                    ax.set_ylim([0, 1])\n",
        "                else:\n",
        "                    ax.text(0.5, 0.5, f'{metric} not available', ha='center', va='center')\n",
        "\n",
        "                ax.set_xlabel('Epoch', fontsize=10)\n",
        "                ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=10)\n",
        "                ax.set_title(f'{cls_name}', fontsize=11, fontweight='bold')\n",
        "                ax.grid(True, alpha=0.3)\n",
        "\n",
        "            # Hide the last subplot\n",
        "            axes[5].axis('off')\n",
        "\n",
        "            plt.suptitle(f'{metric.replace(\"_\", \" \").title()} Over Epochs - {model_name}',\n",
        "                        fontsize=14, fontweight='bold')\n",
        "            plt.tight_layout()\n",
        "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "            plt.close()\n",
        "\n",
        "            print(f\"Saved {metric} curves for all classes to {save_path}\")\n",
        "\n",
        "    def plot_training_summary(self, history, save_path = f'{config.TRAIN_HISTORY_SAVE_PATH}/training_summary.png', model_name='Model'):\n",
        "        \"\"\"\n",
        "        Create a comprehensive summary grid: loss + key macro metrics\n",
        "\n",
        "        Args:\n",
        "            history: Training history dict with 'train_loss', 'val_loss', and metric lists\n",
        "            save_path: Path to save figure\n",
        "            model_name: Name of the model\n",
        "        \"\"\"\n",
        "        if 'train_loss' not in history or not history['train_loss']:\n",
        "            print(\"Warning: 'train_loss' not found. Skipping plot_training_summary.\")\n",
        "            return\n",
        "\n",
        "        epochs = range(1, len(history['train_loss']) + 1)\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        # Plot 1: Loss\n",
        "        axes[0].plot(epochs, history['train_loss'], 'o-', label='Train', linewidth=2)\n",
        "        axes[0].plot(epochs, history.get('val_loss', []), 's-', label='Val', linewidth=2)\n",
        "        axes[0].set_title('Loss', fontweight='bold')\n",
        "        axes[0].set_ylabel('Loss')\n",
        "        axes[0].set_xlabel('Epoch')\n",
        "        axes[0].legend()\n",
        "        axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 2-6: Key macro metrics\n",
        "        key_metrics = [\n",
        "            ('f1', 'F1-Score'),\n",
        "            ('auc_roc', 'AUC-ROC'),\n",
        "            ('sensitivity', 'Sensitivity'),\n",
        "            ('specificity', 'Specificity'),\n",
        "            ('accuracy', 'Accuracy')\n",
        "        ]\n",
        "\n",
        "        for idx, (metric_key, display_name) in enumerate(key_metrics):\n",
        "            ax = axes[idx + 1]\n",
        "            if metric_key in history and history[metric_key]:\n",
        "                ax.plot(epochs, history[metric_key], 'o-', linewidth=2, markersize=5)\n",
        "                ax.set_ylim([0, 1])\n",
        "            ax.set_title(display_name, fontweight='bold')\n",
        "            ax.set_ylabel('Score')\n",
        "            ax.set_xlabel('Epoch')\n",
        "            ax.grid(True, alpha=0.3)\n",
        "\n",
        "        plt.suptitle(f'Training Summary - {model_name}', fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Saved training summary to {save_path}\")\n",
        "\n",
        "    def compare_models_training(self, histories: Dict[str, Dict], metric='f1',\n",
        "                               class_wise_metrics_dict: Dict[str, Dict] = None,\n",
        "                                save_path = f'{config.TRAIN_HISTORY_SAVE_PATH}/model_training_comparison.png'):\n",
        "        \"\"\"\n",
        "        Compare training progress of multiple models\n",
        "\n",
        "        Args:\n",
        "            histories: Dict mapping model_name -> history dict\n",
        "            metric: Metric to compare for macro avg (e.g., 'f1', 'auc_roc')\n",
        "            class_wise_metrics_dict: Optional - Dict mapping model_name -> class_wise_metrics for per-class comparison\n",
        "            save_path: Path to save figure\n",
        "        \"\"\"\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "        # Plot 1: Loss curves\n",
        "        ax = axes[0]\n",
        "        for model_name, history in histories.items():\n",
        "            if 'val_loss' in history and history['val_loss']:\n",
        "                epochs = range(1, len(history['val_loss']) + 1)\n",
        "                ax.plot(epochs, history['val_loss'], 'o-', label=f'{model_name}', linewidth=2, markersize=5)\n",
        "\n",
        "        ax.set_xlabel('Epoch', fontsize=11)\n",
        "        ax.set_ylabel('Validation Loss', fontsize=11)\n",
        "        ax.set_title('Validation Loss Comparison', fontsize=12, fontweight='bold')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "\n",
        "        # Plot 2: Metric curves (macro avg)\n",
        "        ax = axes[1]\n",
        "        for model_name, history in histories.items():\n",
        "            if metric in history and history[metric]:\n",
        "                epochs = range(1, len(history[metric]) + 1)\n",
        "                ax.plot(epochs, history[metric], 'o-', label=f'{model_name}', linewidth=2, markersize=5)\n",
        "\n",
        "        ax.set_xlabel('Epoch', fontsize=11)\n",
        "        ax.set_ylabel(metric.replace('_', ' ').title(), fontsize=11)\n",
        "        ax.set_title(f'Macro-Avg {metric.replace(\"_\", \" \").title()} Comparison',\n",
        "                    fontsize=12, fontweight='bold')\n",
        "        ax.legend()\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_ylim([0, 1])\n",
        "\n",
        "        plt.suptitle('Model Training Comparison', fontsize=14, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
        "        plt.close()\n",
        "\n",
        "        print(f\"Saved model training comparison to {save_path}\")\n",
        "    print(\"  Available: TrainingVisualizer\")"
      ],
      "metadata": {
        "id": "dwnt8VHQ-pyy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOhY_MF38iR3"
      },
      "source": [
        "## 19. Data Preparation\n",
        "\n",
        "**IMPORTANT**: Upload your preprocessed data here!\n",
        "\n",
        "You need to upload your `.npy` batch files to the appropriate directories:\n",
        "- Signals: `data/signals/{train,validation,test}/batch_*.npy`\n",
        "- Images: `data/images/{train,validation,test}/{gaf,mtf}/batch_*.npy`  \n",
        "- Labels: `data/labels/y_{train,val,test}.npy`\n",
        "\n",
        "**Expected shapes:**\n",
        "- Signals: `(batch_size, 1000, 3)` - 1000 timesteps, 3 leads\n",
        "- Images: `(batch_size, 3, 224, 224)` - 3 channels, 224x224 pixels\n",
        "- Labels: `(total_samples, 5)` - one-hot encoded 5 classes\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ack4AEg8iR3"
      },
      "outputs": [],
      "source": [
        "# Verify data is loaded (run after uploading)\n",
        "import numpy as np\n",
        "\n",
        "def check_data():\n",
        "    try:\n",
        "        # Check signals\n",
        "        signal_files = os.listdir('/content/data/signals/train')\n",
        "        print(f\"✓ Found {len(signal_files)} signal batch files\")\n",
        "        if signal_files:\n",
        "            sample = np.load(f'/content/data/signals/train/{signal_files[0]}')\n",
        "            print(f\"  Signal shape: {sample.shape}\")\n",
        "\n",
        "        # Check images\n",
        "        gaf_files = os.listdir('/content/data/images/train/gaf')\n",
        "        print(f\"✓ Found {len(gaf_files)} GAF image batch files\")\n",
        "        if gaf_files:\n",
        "            sample = np.load(f'/content/data/images/train/gaf/{gaf_files[0]}')\n",
        "            print(f\"  GAF image shape: {sample.shape}\")\n",
        "\n",
        "        # Check labels\n",
        "        labels = np.load('/content/data/labels/y_train.npy')\n",
        "        print(f\"✓ Labels shape: {labels.shape}\")\n",
        "\n",
        "        print(\"\\n✓ Data verification complete!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"⚠ Data not found or error: {e}\")\n",
        "        print(\"Please upload your preprocessed .npy files first!\")\n",
        "        return False\n",
        "\n",
        "# Uncomment after uploading data\n",
        "check_data()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DP3aU4m7QjR2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjvJQjOy8iR3"
      },
      "source": [
        "## 20. Training Examples\n",
        "\n",
        "Choose which model you want to train and run the corresponding cell.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMyJxeeR8iR3"
      },
      "source": [
        "### Train RNN Model (Signal-based)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NZ8u16No8iR3",
        "outputId": "ff317ff2-8b41-4a17-db0f-5c8b70f3fed8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=============================================\n",
            "RNN MODEL TRAINING TEST\n",
            "=============================================\n",
            "Device: cuda\n",
            "Batch size: 64\n",
            "Epochs: 10\n",
            "Learning rate: 0.001\n",
            "\n",
            "Loading data...\n",
            "Creating signals dataloaders...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Train batches: 273\n",
            "✓ Batch size: 64\n",
            "✓ Val batches: 35\n",
            "✓ Batch size: 64\n",
            "Training Model: BiGRU\n",
            "\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 273/273 [00:25<00:00, 10.77it/s]\n",
            "Validation: 100%|██████████| 35/35 [00:02<00:00, 16.75it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1691\n",
            "Val Loss: 0.1364\n",
            "Val Metrics (Macro):\n",
            "  Sensitivity: 0.0928\n",
            "  Specificity: 0.9251\n",
            "  Precision: 0.0979\n",
            "  F1-Score: 0.0953\n",
            "  Accuracy: 0.9110\n",
            "  AUC-ROC: nan\n",
            "  AUC-PR: 0.1003\n",
            "âœ“ Saved best model (Val Loss: 0.1364)\n",
            "\n",
            "=============================================\n",
            "Training complete!\n",
            "Best epoch: 1\n",
            "Best val loss: 0.1364\n",
            "=============================================\n",
            "\n",
            "Epoch 2/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/273 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 273/273 [00:25<00:00, 10.83it/s]\n",
            "Validation: 100%|██████████| 35/35 [00:02<00:00, 17.24it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.1305\n",
            "Val Loss: 0.1129\n",
            "Val Metrics (Macro):\n",
            "  Sensitivity: 0.1801\n",
            "  Specificity: 0.9068\n",
            "  Precision: 0.1199\n",
            "  F1-Score: 0.1440\n",
            "  Accuracy: 0.9388\n",
            "  AUC-ROC: nan\n",
            "  AUC-PR: 0.1453\n",
            "âœ“ Saved best model (Val Loss: 0.1129)\n",
            "\n",
            "=============================================\n",
            "Training complete!\n",
            "Best epoch: 2\n",
            "Best val loss: 0.1129\n",
            "=============================================\n",
            "\n",
            "Epoch 3/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/273 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 273/273 [00:24<00:00, 11.26it/s]\n",
            "Validation: 100%|██████████| 35/35 [00:02<00:00, 16.75it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0903\n",
            "Val Loss: 0.0834\n",
            "Val Metrics (Macro):\n",
            "  Sensitivity: 0.1808\n",
            "  Specificity: 0.9455\n",
            "  Precision: 0.1439\n",
            "  F1-Score: 0.1603\n",
            "  Accuracy: 0.9609\n",
            "  AUC-ROC: nan\n",
            "  AUC-PR: 0.1650\n",
            "âœ“ Saved best model (Val Loss: 0.0834)\n",
            "\n",
            "=============================================\n",
            "Training complete!\n",
            "Best epoch: 3\n",
            "Best val loss: 0.0834\n",
            "=============================================\n",
            "\n",
            "Epoch 4/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/273 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 273/273 [00:24<00:00, 10.98it/s]\n",
            "Validation: 100%|██████████| 35/35 [00:02<00:00, 17.01it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0761\n",
            "Val Loss: 0.0789\n",
            "Val Metrics (Macro):\n",
            "  Sensitivity: 0.1852\n",
            "  Specificity: 0.9490\n",
            "  Precision: 0.1475\n",
            "  F1-Score: 0.1642\n",
            "  Accuracy: 0.9648\n",
            "  AUC-ROC: nan\n",
            "  AUC-PR: 0.1685\n",
            "âœ“ Saved best model (Val Loss: 0.0789)\n",
            "\n",
            "=============================================\n",
            "Training complete!\n",
            "Best epoch: 4\n",
            "Best val loss: 0.0789\n",
            "=============================================\n",
            "\n",
            "Epoch 5/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/273 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 273/273 [00:24<00:00, 11.01it/s]\n",
            "Validation: 100%|██████████| 35/35 [00:02<00:00, 17.19it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0702\n",
            "Val Loss: 0.0838\n",
            "Val Metrics (Macro):\n",
            "  Sensitivity: 0.1879\n",
            "  Specificity: 0.9427\n",
            "  Precision: 0.1435\n",
            "  F1-Score: 0.1627\n",
            "  Accuracy: 0.9624\n",
            "  AUC-ROC: nan\n",
            "  AUC-PR: 0.1730\n",
            "\n",
            "=============================================\n",
            "Training complete!\n",
            "Best epoch: 4\n",
            "Best val loss: 0.0789\n",
            "=============================================\n",
            "\n",
            "Epoch 6/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/273 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 273/273 [00:24<00:00, 11.22it/s]\n",
            "Validation: 100%|██████████| 35/35 [00:02<00:00, 16.95it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0692\n",
            "Val Loss: 0.0722\n",
            "Val Metrics (Macro):\n",
            "  Sensitivity: 0.1847\n",
            "  Specificity: 0.9547\n",
            "  Precision: 0.1519\n",
            "  F1-Score: 0.1667\n",
            "  Accuracy: 0.9678\n",
            "  AUC-ROC: nan\n",
            "  AUC-PR: 0.1746\n",
            "âœ“ Saved best model (Val Loss: 0.0722)\n",
            "\n",
            "=============================================\n",
            "Training complete!\n",
            "Best epoch: 6\n",
            "Best val loss: 0.0722\n",
            "=============================================\n",
            "\n",
            "Epoch 7/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/273 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 273/273 [00:24<00:00, 10.94it/s]\n",
            "Validation: 100%|██████████| 35/35 [00:02<00:00, 16.92it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0680\n",
            "Val Loss: 0.0699\n",
            "Val Metrics (Macro):\n",
            "  Sensitivity: 0.1682\n",
            "  Specificity: 0.9699\n",
            "  Precision: 0.1625\n",
            "  F1-Score: 0.1653\n",
            "  Accuracy: 0.9692\n",
            "  AUC-ROC: nan\n",
            "  AUC-PR: 0.1765\n",
            "âœ“ Saved best model (Val Loss: 0.0699)\n",
            "\n",
            "=============================================\n",
            "Training complete!\n",
            "Best epoch: 7\n",
            "Best val loss: 0.0699\n",
            "=============================================\n",
            "\n",
            "Epoch 8/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/273 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 273/273 [00:24<00:00, 11.00it/s]\n",
            "Validation: 100%|██████████| 35/35 [00:02<00:00, 17.03it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0669\n",
            "Val Loss: 0.0729\n",
            "Val Metrics (Macro):\n",
            "  Sensitivity: 0.1778\n",
            "  Specificity: 0.9576\n",
            "  Precision: 0.1529\n",
            "  F1-Score: 0.1644\n",
            "  Accuracy: 0.9664\n",
            "  AUC-ROC: nan\n",
            "  AUC-PR: 0.1747\n",
            "\n",
            "=============================================\n",
            "Training complete!\n",
            "Best epoch: 7\n",
            "Best val loss: 0.0699\n",
            "=============================================\n",
            "\n",
            "Epoch 9/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/273 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 273/273 [00:24<00:00, 11.08it/s]\n",
            "Validation: 100%|██████████| 35/35 [00:02<00:00, 16.44it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0665\n",
            "Val Loss: 0.0761\n",
            "Val Metrics (Macro):\n",
            "  Sensitivity: 0.1875\n",
            "  Specificity: 0.9542\n",
            "  Precision: 0.1520\n",
            "  F1-Score: 0.1679\n",
            "  Accuracy: 0.9687\n",
            "  AUC-ROC: nan\n",
            "  AUC-PR: 0.1762\n",
            "\n",
            "=============================================\n",
            "Training complete!\n",
            "Best epoch: 7\n",
            "Best val loss: 0.0699\n",
            "=============================================\n",
            "\n",
            "Epoch 10/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining:   0%|          | 0/273 [00:00<?, ?it/s]/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n",
            "Training: 100%|██████████| 273/273 [00:24<00:00, 11.20it/s]\n",
            "Validation: 100%|██████████| 35/35 [00:02<00:00, 16.39it/s]\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_classification.py:407: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/metrics/_ranking.py:1033: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.0647\n",
            "Val Loss: 0.0675\n",
            "Val Metrics (Macro):\n",
            "  Sensitivity: 0.1820\n",
            "  Specificity: 0.9617\n",
            "  Precision: 0.1572\n",
            "  F1-Score: 0.1687\n",
            "  Accuracy: 0.9705\n",
            "  AUC-ROC: nan\n",
            "  AUC-PR: 0.1792\n",
            "âœ“ Saved best model (Val Loss: 0.0675)\n",
            "\n",
            "=============================================\n",
            "Training complete!\n",
            "Best epoch: 10\n",
            "Best val loss: 0.0675\n",
            "=============================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'content/training_history/BiGRU_loss_currves.png'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2842575511.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[0;31m# Plot history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_loss_curves\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{config.TRAIN_HISTORY_SAVE_PATH}/{name}_loss_currves.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m     \u001b[0mviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_macro_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{config.TRAIN_HISTORY_SAVE_PATH}/{name}_macro_metrics.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mviz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_training_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf'{config.TRAIN_HISTORY_SAVE_PATH}/{name}_training_history.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2556784132.py\u001b[0m in \u001b[0;36mplot_loss_curves\u001b[0;34m(self, history, save_path, model_name)\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbbox_inches\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'tight'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1241\u001b[0m     \u001b[0;31m# savefig default implementation has no return, so mypy is unhappy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1242\u001b[0m     \u001b[0;31m# presumably this is here because subclasses can return?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1243\u001b[0;31m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[func-returns-value]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1244\u001b[0m     \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_idle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Need this if 'transparent=True', to reset colors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1245\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/figure.py\u001b[0m in \u001b[0;36msavefig\u001b[0;34m(self, fname, transparent, **kwargs)\u001b[0m\n\u001b[1;32m   3488\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3489\u001b[0m                     \u001b[0m_recursively_make_axes_transparent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3490\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcanvas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprint_figure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3492\u001b[0m     def ginput(self, n=1, timeout=30, show_clicks=True,\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36mprint_figure\u001b[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001b[0m\n\u001b[1;32m   2182\u001b[0m                 \u001b[0;31m# force the figure dpi to 72), so we need to set it again here.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2183\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mcbook\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_setattr_cm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2184\u001b[0;31m                     result = print_method(\n\u001b[0m\u001b[1;32m   2185\u001b[0m                         \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2186\u001b[0m                         \u001b[0mfacecolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfacecolor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backend_bases.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2038\u001b[0m                 \"bbox_inches_restore\"}\n\u001b[1;32m   2039\u001b[0m             \u001b[0mskip\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptional_kws\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmeth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2040\u001b[0;31m             print_method = functools.wraps(meth)(lambda *args, **kwargs: meth(\n\u001b[0m\u001b[1;32m   2041\u001b[0m                 *args, **{k: v for k, v in kwargs.items() if k not in skip}))\n\u001b[1;32m   2042\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# Let third-parties do as they see fit.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36mprint_png\u001b[0;34m(self, filename_or_obj, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0;34m*\u001b[0m\u001b[0mmetadata\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincluding\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mdefault\u001b[0m \u001b[0;34m'Software'\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    480\u001b[0m         \"\"\"\n\u001b[0;32m--> 481\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_print_pil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"png\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    482\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    483\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mprint_to_buffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/backends/backend_agg.py\u001b[0m in \u001b[0;36m_print_pil\u001b[0;34m(self, filename_or_obj, fmt, pil_kwargs, metadata)\u001b[0m\n\u001b[1;32m    428\u001b[0m         \"\"\"\n\u001b[1;32m    429\u001b[0m         \u001b[0mFigureCanvasAgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m         mpl.image.imsave(\n\u001b[0m\u001b[1;32m    431\u001b[0m             \u001b[0mfilename_or_obj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_rgba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfmt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morigin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"upper\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    432\u001b[0m             dpi=self.figure.dpi, metadata=metadata, pil_kwargs=pil_kwargs)\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mimsave\u001b[0;34m(fname, arr, vmin, vmax, cmap, format, origin, dpi, metadata, pil_kwargs)\u001b[0m\n\u001b[1;32m   1632\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"format\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1633\u001b[0m         \u001b[0mpil_kwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdefault\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dpi\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1634\u001b[0;31m         \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpil_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1635\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, fp, format, **params)\u001b[0m\n\u001b[1;32m   2581\u001b[0m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"r+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2582\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2583\u001b[0;31m                 \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"w+b\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2584\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2585\u001b[0m             \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mIO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbytes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'content/training_history/BiGRU_loss_currves.png'"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Train RNN with BiGRU + Attention\n",
        "# from signal_dataloader import create_signal_dataloader\n",
        "# from training import train_model\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# config = Config()\n",
        "\n",
        "# Create save directory\n",
        "os.makedirs(config.SAVE_DIR, exist_ok=True)\n",
        "\n",
        "print(\"=\"*45)\n",
        "print(\"RNN MODEL TRAINING TEST\")\n",
        "print(\"=\"*45)\n",
        "print(f\"Device: {config.DEVICE}\")\n",
        "print(f\"Batch size: {config.BATCH_SIZE}\")\n",
        "print(f\"Epochs: {config.NUM_EPOCHS}\")\n",
        "print(f\"Learning rate: {config.LEARNING_RATE}\")\n",
        "\n",
        "config = Config()\n",
        "# Load data\n",
        "print(\"\\nLoading data...\")\n",
        "print(\"Creating signals dataloaders...\")\n",
        "# Create dataloaders\n",
        "train_dataloader = create_signal_dataloader(\n",
        "    image_path=config.TRAIN_SIGNAL_PATH,\n",
        "    labels_path=config.TRAIN_LABEL_PATH,\n",
        "    saved_batch_size=config.SAVED_BATCH_SIZE,\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=config.NUM_WORKERS\n",
        ")\n",
        "data_iter = iter(train_dataloader)\n",
        "batch0 = next(data_iter)\n",
        "# batch0 is a tuple (signals, labels) so use the first element's first dim\n",
        "batch_size_report = batch0[0].size(0) if hasattr(batch0[0], 'size') else len(batch0[0])\n",
        "print(f\"✓ Train batches: {len(train_dataloader)}\")\n",
        "print(f\"✓ Batch size: {batch_size_report}\")\n",
        "val_dataloader = create_signal_dataloader(\n",
        "    image_path=config.VAL_SIGNAL_PATH,\n",
        "    labels_path=config.VAL_LABEL_PATH,\n",
        "    saved_batch_size=config.SAVED_BATCH_SIZE,\n",
        "    batch_size=config.BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=config.NUM_WORKERS\n",
        ")\n",
        "data_iter = iter(val_dataloader)\n",
        "batch0 = next(data_iter)\n",
        "batch_size_report = batch0[0].size(0) if hasattr(batch0[0], 'size') else len(batch0[0])\n",
        "print(f\"✓ Val batches: {len(val_dataloader)}\")\n",
        "print(f\"✓ Batch size: {batch_size_report}\")\n",
        "del data_iter\n",
        "del batch0\n",
        "\n",
        "# Create models to test\n",
        "models_to_train = {\n",
        "    'BiGRU': create_bigru(\n",
        "        hidden_size=config.HIDDEN_SIZE,\n",
        "        num_layers=config.NUM_LAYERS,\n",
        "        dropout=config.DROPOUT\n",
        "    ),\n",
        "    'BiLSTM': create_bilstm(\n",
        "        hidden_size=config.HIDDEN_SIZE,\n",
        "        num_layers=config.NUM_LAYERS,\n",
        "        dropout=config.DROPOUT\n",
        "    ),\n",
        "    'BiGRU_AddAttention': create_bigru_attention(\n",
        "        hidden_size=config.HIDDEN_SIZE,\n",
        "        num_layers=config.NUM_LAYERS,\n",
        "        dropout=config.DROPOUT,\n",
        "        attention_type='additive'\n",
        "    ),\n",
        "    'BiGRU_SelfAttention': create_bigru_attention(\n",
        "        hidden_size=config.HIDDEN_SIZE,\n",
        "        num_layers=config.NUM_LAYERS,\n",
        "        dropout=config.DROPOUT,\n",
        "        attention_type='self'\n",
        "    )\n",
        "    # 'CRNN_1D': CRNN_1D(3, 5, 'gru', 128, 2, True, 0.4)\n",
        "}\n",
        "# Train each model\n",
        "histories = []\n",
        "model_names = []\n",
        "\n",
        "# Loss function (Binary Cross Entropy for multi-label)\n",
        "criterion = nn.BCEWithLogitsLoss()\n",
        "metrics = ECGMetrics(config.CLASS_NAMES)\n",
        "viz = TrainingVisualizer(config.CLASS_NAMES)\n",
        "\n",
        "for name, model in models_to_train.items():\n",
        "    print(f\"Training Model: {name}\")\n",
        "    # Optimizer\n",
        "    model = model.to(config.DEVICE)\n",
        "    optimizer = optim.Adam(model.parameters(),\n",
        "                           lr=config.LEARNING_RATE,\n",
        "                           weight_decay=config.WEIGHT_DECAY)\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3,\n",
        "        min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    history, class_wise_metrics = train_model(\n",
        "        model,\n",
        "        name,\n",
        "        train_dataloader,\n",
        "        val_dataloader,\n",
        "        criterion,\n",
        "        optimizer,\n",
        "        scheduler,\n",
        "        metrics,\n",
        "        config,\n",
        "    )\n",
        "    histories.append(history)\n",
        "    model_names.append(name)\n",
        "\n",
        "    # Plot history\n",
        "    viz.plot_loss_curves(history, f'{config.TRAIN_HISTORY_SAVE_PATH}/{name}_loss_currves.png', name)\n",
        "    viz.plot_macro_metrics(history, f'{config.TRAIN_HISTORY_SAVE_PATH}/{name}_macro_metrics.png', name)\n",
        "    viz.plot_training_summary(history, f'{config.TRAIN_HISTORY_SAVE_PATH}/{name}_training_history.png', name)\n",
        "\n",
        "    # Per-class metrics (requires class_wise_metrics)\n",
        "    viz.plot_per_class_metric(class_wise_metrics, save_path = f'{config.TRAIN_HISTORY_SAVE_PATH}/{name}_per_class_metric.png', name=name)\n",
        "    viz.plot_per_class_metric(class_wise_metrics, )  # All classes\n",
        "\n",
        "# Save histories to disk for future analysis\n",
        "print(\"\\nSaving training histories...\")\n",
        "save_histories(histories, model_names)\n",
        "\n",
        "# Plot results\n",
        "plot_training_history(histories, model_names)\n",
        "\n",
        "print(\"\\n\" + \"=\"*45)\n",
        "print(\"✓ ALL TRAINING COMPLETE\")\n",
        "print(\"=\"*45)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqS0MgXW8iR3"
      },
      "source": [
        "### Example 2: Train CNN2D AlexNet (Image-based)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5RLoIwCL8iR3"
      },
      "outputs": [],
      "source": [
        "# Train AlexNet with CBAM attention\n",
        "# from image_dataloader import create_image_dataloader\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = create_image_dataloader(\n",
        "    config.TRAIN_IMAGE_PATH, config.TRAIN_LABEL_PATH, image_type='gaf',\n",
        "    batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS\n",
        ")\n",
        "\n",
        "val_loader = create_image_dataloader(\n",
        "    config.VAL_IMAGE_PATH, config.VAL_LABEL_PATH, image_type='gaf',\n",
        "    batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS\n",
        ")\n",
        "\n",
        "# Create model\n",
        "alexnet_model = create_alexnet_cbam(num_classes=config.NUM_CLASSES, dropout=config.DROPOUT)\n",
        "\n",
        "# Train\n",
        "alexnet_history = train_model(\n",
        "    model=alexnet_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    config=config,\n",
        "    model_name='AlexNet_CBAM'\n",
        ")\n",
        "\n",
        "print(\"\\n✓ AlexNet training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QeHmiJv8iR3"
      },
      "source": [
        "### Example 3: Train ResNet\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BteNjjTW8iR3"
      },
      "outputs": [],
      "source": [
        "# Train ResNet\n",
        "# from image_dataloader import create_image_dataloader\n",
        "\n",
        "# Create dataloaders (reuse from above or create new)\n",
        "train_loader = create_image_dataloader(\n",
        "    config.TRAIN_IMAGE_PATH, config.TRAIN_LABEL_PATH, image_type='gaf',\n",
        "    batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS\n",
        ")\n",
        "\n",
        "val_loader = create_image_dataloader(\n",
        "    config.VAL_IMAGE_PATH, config.VAL_LABEL_PATH, image_type='gaf',\n",
        "    batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS\n",
        ")\n",
        "\n",
        "# Create model\n",
        "resnet_model = ResNet(num_classes=config.NUM_CLASSES)\n",
        "\n",
        "# Train\n",
        "resnet_history = train_model(\n",
        "    model=resnet_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    config=config,\n",
        "    model_name='ResNet'\n",
        ")\n",
        "\n",
        "print(\"\\n✓ ResNet training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oot4lTdZ8iR3"
      },
      "source": [
        "### Example 4: Train Joint Fusion Model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It1Vsi9H8iR3"
      },
      "outputs": [],
      "source": [
        "# Train Fusion Model combining Signal + Image\n",
        "# from fusion_dataloader import create_fusion_dataloader\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = create_fusion_dataloader(\n",
        "    config.TRAIN_SIGNAL_PATH, config.TRAIN_IMAGE_PATH, config.TRAIN_LABEL_PATH,\n",
        "    image_type='gaf', batch_size=config.BATCH_SIZE, shuffle=True, num_workers=config.NUM_WORKERS\n",
        ")\n",
        "\n",
        "val_loader = create_fusion_dataloader(\n",
        "    config.VAL_SIGNAL_PATH, config.VAL_IMAGE_PATH, config.VAL_LABEL_PATH,\n",
        "    image_type='gaf', batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS\n",
        ")\n",
        "\n",
        "# Create signal branch\n",
        "signal_branch = create_bigru_attention(\n",
        "    hidden_size=config.HIDDEN_SIZE,\n",
        "    num_layers=config.NUM_LAYERS,\n",
        "    dropout=config.DROPOUT\n",
        ")\n",
        "\n",
        "# Create image branch\n",
        "image_branch = create_alexnet(num_classes=config.NUM_CLASSES, dropout=config.DROPOUT)\n",
        "\n",
        "# Create fusion model\n",
        "fusion_model = JointFusion(\n",
        "    signal_model=signal_branch,\n",
        "    image_model=image_branch,\n",
        "    num_classes=config.NUM_CLASSES,\n",
        "    fusion_dim=128\n",
        ")\n",
        "\n",
        "# Train\n",
        "fusion_history = train_model(\n",
        "    model=fusion_model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    config=config,\n",
        "    model_name='JointFusion'\n",
        ")\n",
        "\n",
        "print(\"\\n✓ Fusion model training completed!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Zgjk9P18iR3"
      },
      "source": [
        "## 21. Evaluation on Test Set\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ERzMLiFP8iR3"
      },
      "outputs": [],
      "source": [
        "# Load and evaluate any trained model\n",
        "import torch\n",
        "# from metrics import ECGMetrics\n",
        "\n",
        "def load_and_evaluate(model, model_name, test_loader, config):\n",
        "    \"\"\"Load best checkpoint and evaluate\"\"\"\n",
        "    device = config.DEVICE\n",
        "    checkpoint_path = os.path.join(config.SAVE_DIR, f\"{model_name}_best.pth\")\n",
        "\n",
        "    if os.path.exists(checkpoint_path):\n",
        "        checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        print(f\"✓ Loaded checkpoint: {checkpoint_path}\")\n",
        "        print(f\"  Best epoch: {checkpoint['epoch']+1}\")\n",
        "        print(f\"  Validation loss: {checkpoint['val_loss']:.4f}\")\n",
        "    else:\n",
        "        print(f\"⚠ No checkpoint found, using current model weights\")\n",
        "\n",
        "    # Evaluate\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            if isinstance(inputs, (list, tuple)):\n",
        "                inputs = [inp.to(device) for inp in inputs]\n",
        "            else:\n",
        "                inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            if isinstance(outputs, tuple):\n",
        "                outputs = outputs[0]  # Handle attention models\n",
        "\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, predicted = outputs.max(1)\n",
        "\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "    y_true = np.array(all_labels)\n",
        "    y_pred = np.array(all_preds)\n",
        "    y_prob = np.array(all_probs)\n",
        "\n",
        "    # Calculate metrics\n",
        "    metrics_calc = ECGMetrics()\n",
        "    metrics = metrics_calc.calculate_metrics(y_true, y_pred, y_prob)\n",
        "\n",
        "    # Print report\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(f\"EVALUATION RESULTS - {model_name}\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
        "    print(f\"F1 Score (Macro): {metrics['f1_macro']:.4f}\")\n",
        "    print(f\"F1 Score (Weighted): {metrics['f1_weighted']:.4f}\")\n",
        "    if 'auc_roc_macro' in metrics:\n",
        "        print(f\"AUC-ROC (Macro): {metrics['auc_roc_macro']:.4f}\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    metrics_calc.plot_confusion_matrices(\n",
        "        [(metrics['confusion_matrix'], model_name)],\n",
        "        save_path=f'results/{model_name}_confusion_matrix.png'\n",
        "    )\n",
        "\n",
        "    # Plot ROC curves\n",
        "    metrics_calc.plot_roc_curves(\n",
        "        y_true, y_prob,\n",
        "        title=f'{model_name} - ROC Curves',\n",
        "        save_path=f'results/{model_name}_roc_curves.png'\n",
        "    )\n",
        "\n",
        "    return metrics\n",
        "\n",
        "print(\"✓ Evaluation function ready\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4zt0o0M8iR3"
      },
      "outputs": [],
      "source": [
        "# Example: Evaluate RNN model\n",
        "from signal_dataloader import create_signal_dataloader\n",
        "\n",
        "test_loader = create_signal_dataloader(\n",
        "    config.TEST_SIGNAL_PATH, config.TEST_LABEL_PATH,\n",
        "    batch_size=config.BATCH_SIZE, shuffle=False, num_workers=config.NUM_WORKERS\n",
        ")\n",
        "\n",
        "# Uncomment after training\n",
        "# rnn_results = load_and_evaluate(rnn_model, 'RNNModel', test_loader, config)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9TkUFTiS8iR4"
      },
      "source": [
        "## 22. Download Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mNZ4j5ss8iR4"
      },
      "outputs": [],
      "source": [
        "# Zip and download all results\n",
        "import shutil\n",
        "\n",
        "# Create archives\n",
        "shutil.make_archive('ecg_results', 'zip', 'results')\n",
        "shutil.make_archive('ecg_checkpoints', 'zip', config.SAVE_DIR)\n",
        "\n",
        "print(\"✓ Archives created:\")\n",
        "print(\"  - ecg_results.zip\")\n",
        "print(\"  - ecg_checkpoints.zip\")\n",
        "\n",
        "# Download if on Colab\n",
        "if IN_COLAB:\n",
        "    from google.colab import files\n",
        "    files.download('ecg_results.zip')\n",
        "    files.download('ecg_checkpoints.zip')\n",
        "    print(\"✓ Downloads started\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xd1HrUPf8iR4"
      },
      "source": [
        "## 23. Tips & Troubleshooting\n",
        "\n",
        "### Common Issues:\n",
        "\n",
        "**Out of Memory:**\n",
        "- Reduce `BATCH_SIZE` (try 16 or 8)\n",
        "- Reduce `NUM_WORKERS` to 0\n",
        "- Use `torch.cuda.empty_cache()`\n",
        "\n",
        "**Data Not Found:**\n",
        "- Verify you uploaded .npy files to correct directories\n",
        "- Check file shapes match expected dimensions\n",
        "- Run `check_data()` function\n",
        "\n",
        "**Training Not Converging:**\n",
        "- Try different learning rates\n",
        "- Check data normalization\n",
        "- Increase epochs\n",
        "- Reduce dropout\n",
        "\n",
        "**Colab Disconnection:**\n",
        "- Save checkpoints to Google Drive\n",
        "- Consider Colab Pro for longer runtime\n",
        "- Use `%%capture` to suppress long outputs\n",
        "\n",
        "### Performance Tips:\n",
        "- Enable GPU: Runtime → Change runtime type → GPU (T4 or better)\n",
        "- Monitor GPU: Run `!nvidia-smi` in a cell\n",
        "- Use mixed precision training for speed (add to training.py)\n",
        "- Reduce image size if memory constrained\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}